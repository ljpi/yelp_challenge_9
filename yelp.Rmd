---
title: 'Modeling Restaurant-Goer''s Behavior in The Great Recession: An Econometric
  Case Study'
author: "Lester Pi"
date: "4/12/2017"
output:
  html_document: default
  latex_engine: xelatex
  pdf_document: default
---

```{r, echo=FALSE, include=FALSE}
library(jsonlite)
library('vars')
library('readxl')
library('Quandl')
library('tseries')
library("forecast")
library("quantmod")
library("xts")
library("tis")
library("moments")
library('stats')
library("strucchange")
library(knitr)
library(rmarkdown)
library(sqldf)
library("hashmap")
library(stringr)
library(fpp)
library(tm)
library(SnowballC)
library(wordcloud)
library(nlme)
library(tidyr)
library(dplyr)
library(tidytext)
library(ggplot2)
library(car)
```

```{r global_options, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.path='Figs/')
```

```{r setup, include=FALSE}
opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
opts_chunk$set(dev = 'pdf')
```


#Abstract

This is a case study into Yelp restaurant-goers' consumer behavior during The Great Recession based off data from the Yelp Dataset Challenge 9. By manipulating and transforming the dataset, I rebuilt the relevant data into an econometric framework. Combined with a very light semantic analysis, we are able to see how consumer behavior changed during The Great Recession. With this information and econometric models, we can effectively determine how restaurant-goers' behavior will change in the event of a future recession.

#Introduction

Yelp is a platform where users can review businesses based off a star system, with one star being the lowest and five stars being the highest. Along with the review, users write their thoughts about it which typically include why they feel the business earned the score they gave it. Another key feature of Yelp is business information that includes attributes such as price, business type, and location.

The Great Recession hit in December 2007 and lasted until June 2009 and was related to the financial crisis of 2007-08 and subprime mortgage crisis of 2007-09. One of the key aspects from a consumer standpoint is the Consumer Price Index (CPI), which is an indexed measure of prices and purchasing power. According to the Federal Reserve Economic Data of St. Louis (FRED), the CPI for urban food and beverages increases during the most of the recession, but declines towards the end. In other words, purchasing power was weaker during most of the recession.

#Motivation

I wanted to focus on the restaurant industry for three main reasons: Yelp is very well-known for their restaurant reviews, I majoritively use Yelp for restaurant reviews, and I have a personal interest in the food and restaurant world. The Great Recession sets the stage for a great case study in that it was recent enough to occur after Yelp's conception and this also allows to account for systematic time differences. It was also a very interesting recession since it had huge ramifications both domestically and worldwide.

If restaurant Yelpers' behavior during and around The Great Recession period can be modeled, then we can apply this model to a future recession. Depending on the results, this can have important insights for restaurants who are looking to survive, or perhaps even take advantage of, a recession.


#Datasets

The following are the datasets which I used in this case study.

Yelp Dataset Challenge 9: Contains a selective subset of Yelp data covering reviews, users, businesses, tips, and check-ins. Core datasets that will be examined.

FRED GDP: U.S. Real GDP data pulled from FRED. Used to examine The Great Recession.

Yahoo Finance Yelp Stock: Yelp's adjusted closure stock price. Used to measure Yelp company success and performance.

BEA Restaurant Expenditures: Seasonally adjusted real restaurant expenditures from The BEA. Used to connect Yelp restaurant data with a generalized restaurant industry.

#Data Work Oveview

The Yelp data came in large Json files that needed to be converted into R-workable dataframes. Once in a workable format, I explored the data and extracted the relevant information. Regular expressions were used to analyze large groups of text when only a small specific portion was needed. By using SQL queries, I subsetted the interesting data into the needed date ranges and the associated categories, such as by geography and business type.

The other data sets were pulled from their sources, either manually or through R, and formatted and transformed accordingly.

Most of the data is either transformed into growth rates, detrended and/or seasonally adjusted, or kept in original levels.


#Challenges and Data Issues Addressed

1. Yelp Data has a disproportionate amount of observations during its early days as well as the latest month due to not having a complete month's worth of data. This can cause statistical insignificance and heteroskedasticity. In order to prevent this, I omitted some of the earliest and latest data.

2. Almost everything is a affected by endogeneity. I tried to prove or disprove what I thought could be a potential instrumental variable that could be used to reduce endogeneity.

3. The Yelp data is a Yelp-decided subset of their data. This can cause large selection bias. I examined the data and saw that it includes small, medium, and large cities alike. There is no way to obtain the unreleased portion of data.

4. Level sets vs. growth rates. Growth rates allow our data to be transformed into stationary (or nearly stationary) data. However, they do not always make intuitive sense for this case study. Therefore, I used the levels for creating linear models, but used growth rates for determining Granger causality and VAR models.


```{r chunk1, echo=FALSE, warning=FALSE}
#SETUP
setwd("C:/cygwin64/home/Lester/yelp_challenge_9")

par(xpd=NA)

load_json = function(filename){
  json_file = file(filename)
  json_data = jsonlite::stream_in(json_file)
  return(json_data)
}



remove_lists_from_df = function(df){
  i=1
  while(i <= length(df)){
    if(class(df[,i])=="list"){
      df[i] = sapply(df[,i], paste, collapse="|")
    }
    i=i+1
  }
  
  return(df)
}


add_recession_dummy = function(l){
  rec=c()
  for(i in 1:length(l)){
    
    if(l[i]>=as.Date("2007-12-01")&l[i]<=as.Date("2009-07-01")){
      rec=c(rec,1)
    }
    else{
      rec=c(rec,0)
    }
  }
  
  return(rec)
  
}


descriptive_stats = function(lm_mod, name){
  par(mfrow=c(3,2))
  rec = recresid(resid(lm_mod)~1,col="skyblue3")
  plot(resid(lm_mod))
  truehist(resid(lm_mod))
  print(resettest(lm_mod))
  acf(resid(lm_mod))
  pacf(resid(lm_mod))
  jarque.bera.test(resid(lm_mod))
  plot(efp(resid(lm_mod)~1,type="Rec-CUSUM"))
  plot(rec,pch=16)
  abline(h=0,col="red")
  title(paste("Descriptive Statistics:",name,sep=" "),outer=TRUE)
}

test_stationary = function(t){
  print(kpss.test(t))
  print(adf.test(t))
}

test_cointegration = function(resid){
  print(test_stationary(resid))
}

#dollar sign extractor
get_dollar_signs=function(s){
  temp=str_extract(s,"RestaurantsPriceRange2(.*?)[0-9]")
  return(substr(temp,nchar(temp),nchar(temp)))
}

buildCorpus = function(data, stem){
  corpus = Corpus(VectorSource(data))
  corpus = tm_map(corpus, content_transformer(tolower))
  corpus = tm_map(corpus, PlainTextDocument)
  corpus = tm_map(corpus, removePunctuation)
  corpus = tm_map(corpus, removeWords, stopWords)
  if(stem==1) corpus = tm_map(corpus, stemDocument)
  return(corpus)
}

buildWordCloud = function(corpus, pal,val, name){
  wordcloud(corpus, max.words = 75, random.order = FALSE, colors=brewer.pal(val,pal), main = name)
}

```


#Does Yelp Performance Affect the Userbase?

First, I will explore the Yelp stock data to see if it is of any relevance in an attempt to handle any cases of endogeneity. In this case, the stock data will be a representation of the company performance. Intuitively, there is a chance it may be an instrumental variable such that the better the company is doing, the more they can advertise to, accumulate, and support a larger userbase.

```{r chunk2, echo=FALSE, include=FALSE, message=FALSE}
user = load_json("yelp_academic_dataset_user.json")
colnames(user)
subset_users = user[4]
# subset_users
users_by_date_all = sqldf("select yelping_since, count(yelping_since) from subset_users group by yelping_since order by yelping_since")


#usethis
users_by_date = with(users_by_date_all, users_by_date_all[(users_by_date_all$yelping_since >= "2006" & users_by_date_all$yelping_since < "2017"), ])

users_by_date_xts=xts(users_by_date$`count(yelping_since)`, as.Date(users_by_date$yelping_since,"%Y-%m-%d"))
ts_m = apply.monthly(users_by_date_xts, sum)

# ts_m


users_df = data.frame(date=index(ts_m), coredata(ts_m))
# users_df

stock_users = with(users_df, users_df[(users_df$date >= "2012-02-01"), ])
```


```{r chunk3, echo=FALSE, warning=FALSE}
plot(stock_users,type="l",ylab="New Users",xlab="Date", main= "New User Acounts by Month")

yelp_stock = get.hist.quote("YELP",end="2017-01-30",quote="AdjClose",compression="m")
# yelp_stock

plot(yelp_stock,ylab= "Adjusted Close",xlab="Date",main="Yelp Stock")

#test_stationary(ts(stock_users$coredata.ts_m.,start=c(2012,3),freq=12))
#test_stationary(yelp_stock)

log_user_growth = as.data.frame(diff(log(stock_users$coredata.ts_m.)))

log_yelp_growth = as.data.frame(diff(log(yelp_stock)))

ts_users = ts(log_user_growth,start=c(2012,4),freq=12)
#test_stationary(ts_users)

ts_yelp = ts(log_yelp_growth,start=c(2012,4),freq=12)
#test_stationary(ts_yelp)

combined = cbind(ts_yelp,ts_users)
select=VARselect(combined,lag.max=12,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm=VAR(combined,p=select$select[1])
# plot(vm$y)
summary(vm)

grangertest(ts_users~ts_yelp,order=select$select[1])
grangertest(ts_yelp~ts_users,order=select$select[1])


```

The number of new users do not have an effect on the stock value of Yelp and vice versa. This allows to eliminate the stock value as an instrumental variable.



```{r chunk4, echo=FALSE, warning=FALSE}
#remove user data to save memory

rm(user)
rm(subset_users)
rm(users_by_date_all)
rm(users_by_date)
rm(users_by_date_xts)
rm(users_df)
rm(stock_users)

```

#Connecting Users and Reviews

We will attempt to connect new users with new reviews.

```{r chunk5, echo=FALSE, include=FALSE, message=FALSE}

review = load_json("yelp_academic_dataset_review.json")

#join with restaurant reviews
business = load_json("yelp_academic_dataset_business.json")
business = remove_lists_from_df(business)

#sanity check the states for subsetting into states
sanity_state = sqldf("select state from business group by state")
# sanity_state

#us only
restaurants = sqldf("select business_id from business where (categories like '%Restaurants%' and (state = 'AZ' or state = 'IL' or state = 'NC' or state = 'NV' or state = 'NY' or state = 'OH' or state = 'PA' or state = 'SC' or state = 'VT' or state = 'WI'))")

#us only
restaurant_reviews_all = sqldf("select * from review join restaurants on review.business_id = restaurants.business_id")
colnames(restaurant_reviews_all) = c("review_id","user_id","b_id","stars","date","text","useful","funny","cool","type","business_id")

#use this for reviews, start from 2006 for larger sample sizes (<2006 extremely low sample sizes & heterskedasticity)
restaurant_reviews = with(restaurant_reviews_all, restaurant_reviews_all[(restaurant_reviews_all$date >= "2006" & restaurant_reviews_all$date < "2017"), ])

rm(review)
rm(sanity_state)
rm(restaurants)
rm(restaurant_reviews_all)

review_date_star = as.data.frame(restaurant_reviews$date)
review_date_star$stars = restaurant_reviews$stars
# head(review_date_star)
colnames(review_date_star)=c("date","stars")


review_date_star$date=as.Date(review_date_star$date,"%Y-%m-%d")

#freq of new reviews
review_counts_by_date = sqldf("select date, count(date) from review_date_star group by date order by date")

# head(review_counts_by_date)

```


```{r chunk6, echo=FALSE, warning=FALSE}
# plot(review_counts_by_date$date,review_counts_by_date$`count(date)`,type="l")

#convert to monthly
reviews_by_date=xts(review_counts_by_date$`count(date)`, as.Date(review_counts_by_date$date,"%Y-%m-%d"))
df_rev_m = apply.monthly(reviews_by_date, sum)
df_rev_count = data.frame(date=index(df_rev_m), coredata(df_rev_m))
# df_rev_count

plot(df_rev_count$date,df_rev_count$coredata.df_rev_m.,type="l",xlab="Date",ylab="Monthly Reviews",main="New Reviews by Month")


#review growth rates/new user growth rates
log_rev_count=diff(log(df_rev_count$coredata.df_rev_m.))
#log_rev_count[1]=NA
log_rev_count = na.omit(log_rev_count)
log_rev_count = ts(log_rev_count,start=c(2006,2),freq=12)

# ts_m
log_user_count=diff(log(ts_m[,1]))
log_user_count = na.omit(log_user_count)
log_user_count = ts(log_user_count,start=c(2006,2),freq=12)

# plot(log_rev_count,type="l", main="growth rate of user reviews and accounts")
# lines(log_user_count[,1],col="red")

#do a var model between growth rate of users revs and accounts

#create var of growth rates
rates_combined = cbind(log_rev_count,log_user_count)
select=VARselect(rates_combined,lag.max=12,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_rates=VAR(rates_combined,select$select[1])
# plot(vm_rates$y)
summary(vm_rates)

#do granger causality test
grangertest(log_rev_count~log_user_count,order=select$select[1])
grangertest(log_user_count~log_rev_count,order=select$select[1])

user_rev_lm=lm(log_rev_count~log_user_count)
summary(user_rev_lm)

#descriptive_stats(user_rev_lm, "User Reviews on User Counts (dlog)")

#test_cointegration(resid(user_rev_lm))

#users and num reviews granger cause each other and are cointegrated, just use one

```

Through the VAR model results, Granger causality test, and cointegration test, we can conclude that users and reviews can be used interchangably. The error descriptive statistics (Note: error descriptive statistics surpressed in writeup) do not look perfect, but we just want to see if reviews can be used in place of users. From here on out, we will focus on reviews since reviews contain more valuable information than the user data.

#Do Review Scores Change?

By examining review scores, we can see if people become more or less critical during the recession, and many other insights.

```{r chunk7, echo=FALSE, warning=FALSE}


#stars by month
stars_by_date=xts(review_date_star$stars, as.Date(review_date_star$date,"%Y-%m-%d"))
df_stars_m = apply.monthly(stars_by_date, sum)
df_stars = data.frame(date=index(df_stars_m), coredata(df_stars_m))


df_stars$avg = df_stars$coredata.df_stars_m./df_rev_count$coredata.df_rev_m.
# head(df_stars)

par(mfrow=c(1,1))
plot(df_stars$date,df_stars$avg,type = "l",xlab="Date",ylab="Average Review Scores",main="Average Review Scores by Month")
#evidence of recession in stars
stars_recession_dummy = add_recession_dummy(df_stars$date)
stars_avg=df_stars$avg
stars_lm=lm(stars_avg~stars_recession_dummy)
summary(stars_lm)
#descriptive_stats(stars_lm, "Average Stars on Recession")



#test_stationary(df_stars$avg)
#convert to growth rates
df_stars_diff_log = as.data.frame(diff(log(df_stars$avg)))
df_stars_diff_log$date=df_stars$date[2:length(df_stars$date)]

# df_stars_diff_log
colnames(df_stars_diff_log)=c("avg","date")

#test_stationary(df_stars_diff_log$avg)


stars_recession_diff_log_dummy = add_recession_dummy(df_stars_diff_log$date)

# stars_reg = lm(df_stars_diff_log$avg ~ stars_recession_diff_log_dummy)
# summary(stars_reg)

# plot(df_stars_diff_log$date,df_stars_diff_log$avg,type="l",main="Growth Rate of Review Scores by Month",xlab="Date",ylab="Review Scores Growth Rate")
# 
# stars_diff_log_avg = df_stars_diff_log$avg
# stars_diff_log_lm = lm(stars_diff_log_avg~stars_recession_diff_log_dummy)
# summary(stars_diff_log_lm)
# #descriptive_stats(stars_diff_log_lm)

#however, intuitively we should be looking at level, not growth rates
#so lets detrend the data and season

ts_stars = ts(df_stars$avg,start=c(2006,2),freq=12)

stars_tslm = tslm(ts_stars~trend+season)
summary(stars_tslm)


# plot(ts_stars)
# lines(stars_tslm$fitted.values,col="red")

detrend_stars = resid(stars_tslm)
par(mfrow=c(1,1))
plot(detrend_stars, main="Detrended & Seasonally Adjusted Review Scores by Month",ylab = "Average Review Score")

detrend_stars_lm=lm(detrend_stars~stars_recession_dummy)
summary(detrend_stars_lm)
#descriptive_stats(detrend_stars_lm, "Average Stars Adjusted on Recession")




```

The regression results cannot be trusted because the error statistics are all over the place. No conclusion can be made yet.

```{r chunk8, echo=FALSE, warning=FALSE}

#real gdp
getSymbols('GDPC96', src = 'FRED')
gdp = GDPC96

plot(gdp,main="Real GDP")

gdp_growth = na.omit(diff(log(gdp)))
plot(gdp_growth,ylab="GDP Growth",main="Real GDP Growth Rate")


gdp_growth_subset = with(gdp_growth, gdp_growth[index(gdp_growth) >= "2006-04-01" & index(gdp_growth) < "2016-12-30", ])
gdp_growth_subset = ts(gdp_growth_subset,start=c(2006,2),frequency = 4)
plot(gdp_growth_subset,ylab="GDP Growth",main="Real GDP Growth 2006Q2 to 2016Q4")
rect(2007.9166667, -1, 2009.5, 1, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)

#create quarterly review growth rate
reviews_by_quarter=xts(review_counts_by_date$`count(date)`, as.Date(review_counts_by_date$date,"%Y-%m-%d"))
df_rev_m_quarter = apply.quarterly(reviews_by_quarter, sum)
df_rev_quarter = data.frame(date=index(df_rev_m_quarter), coredata(df_rev_m_quarter))
# df_rev_quarter
# plot(df_rev_quarter$date,df_rev_quarter$coredata.df_rev_m_quarter.,type="l")

log_rev_quarter=diff(log(df_rev_quarter$coredata.df_rev_m_quarter.))
log_rev_quarter = na.omit(log_rev_quarter)
log_rev_quarter = ts(log_rev_quarter,start=c(2006,2),freq=4)
plot(log_rev_quarter,type="l",ylab="Review Growth Rate",main="Review Growth Rate by Quarter")
rect(2007.9166667, -1, 2009.5, 1, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)

# length(log_rev_quarter)
# length(gdp_growth_subset)

#var of gdp and user reviews growth rates
gdp_combined = cbind(log_rev_quarter,gdp_growth_subset)
select=VARselect(gdp_combined,lag.max=12,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_gdp=VAR(gdp_combined,p=12)
# plot(vm_gdp$y)
summary(vm_gdp)


recession_dummy_reviews_q = add_recession_dummy(df_rev_quarter$date)

reg_reviews = lm(df_rev_quarter$coredata.df_rev_m_quarter.~recession_dummy_reviews_q)
summary(reg_reviews)
#descriptive_stats(reg_reviews, "Quartly Reviews on Recession")
#first reg is misleading because trend



#look at growth rates

rec_dummy_rev_growth_q = recession_dummy_reviews_q[2:length(recession_dummy_reviews_q)]

reg_log_reviews = lm(log_rev_quarter~rec_dummy_rev_growth_q)
summary(reg_log_reviews)
#descriptive_stats(reg_log_reviews, "Quarterly Reviews on Recession (log)")

```

It looks that using the growth rates are better suited based off the error descriptive statistics, but we want to examine the social dynamics and not reviews as a whole.

Let's continue by splitting the restaurants by dollar signs (\$).

#Examining Restaurants by Prices

How do people react to prices during a recession? To examine this, we will subset our data into 4 categories by price, one for each price level. This will be refered to as how many dollar signs (\$) a business is.

```{r chunk9, echo=FALSE, include=FALSE, message=FALSE}
#convert $ dollar signs to time series
restaurant_visits = sqldf("select restaurant_reviews.date, business.attributes, restaurant_reviews.stars from restaurant_reviews join business on restaurant_reviews.business_id = business.business_id")

restaurant_dollars=data.frame(restaurant_visits$date)
restaurant_dollars$dollars = get_dollar_signs(restaurant_visits$attributes)

restaurant_dollars=na.omit(restaurant_dollars)
class(restaurant_dollars$restaurant_visits.date)
restaurant_dollars$restaurant_visits.date=as.character(restaurant_dollars$restaurant_visits.date)
colnames(restaurant_dollars) = c("date","dollars")

dollars_1 = sqldf("select * from restaurant_dollars where dollars=1")
dollars_2 = sqldf("select * from restaurant_dollars where dollars=2")
dollars_3 = sqldf("select * from restaurant_dollars where dollars=3")
dollars_4 = sqldf("select * from restaurant_dollars where dollars=4")

dollars_gbd_1 = sqldf("select date, count(date) from dollars_1 group by date order by date")
dollars_gbd_2 = sqldf("select date, count(date) from dollars_2 group by date order by date")
dollars_gbd_3 = sqldf("select date, count(date) from dollars_3 group by date order by date")
dollars_gbd_4 = sqldf("select date, count(date) from dollars_4 group by date order by date")

```

```{r chunk10, echo=FALSE, warning=FALSE}

dollars_1_xts=xts(dollars_gbd_1$`count(date)`, as.Date(dollars_gbd_1$date,"%Y-%m-%d"))
df_d_1 = apply.monthly(dollars_1_xts, sum)
df_dollars_1 = data.frame(date=index(df_d_1), coredata(df_d_1))
# df_dollars_1

dollars_2_xts=xts(dollars_gbd_2$`count(date)`, as.Date(dollars_gbd_2$date,"%Y-%m-%d"))
df_d_2 = apply.monthly(dollars_2_xts, sum)
df_dollars_2 = data.frame(date=index(df_d_2), coredata(df_d_2))
# df_dollars_2

dollars_3_xts=xts(dollars_gbd_3$`count(date)`, as.Date(dollars_gbd_3$date,"%Y-%m-%d"))
df_d_3 = apply.monthly(dollars_3_xts, sum)
df_dollars_3 = data.frame(date=index(df_d_3), coredata(df_d_3))
# df_dollars_3

dollars_4_xts=xts(dollars_gbd_4$`count(date)`, as.Date(dollars_gbd_4$date,"%Y-%m-%d"))
df_d_4 = apply.monthly(dollars_4_xts, sum)
df_dollars_4 = data.frame(date=index(df_d_4), coredata(df_d_4))
# df_dollars_4


recession_dummy_dollars_m = add_recession_dummy(df_dollars_1$date)
# 
# par(mfrow=c(2,2))
# plot(df_dollars_1,type="l",xlab="Date",ylab="New Reviews", main="New Reviews of $ Restaurants")
# plot(df_dollars_2,type="l",xlab="Date",ylab="New Reviews", main="New Reviews of $$ Restaurants")
# plot(df_dollars_3,type="l",xlab="Date",ylab="New Reviews", main="New Reviews of $$$ Restaurants")
# plot(df_dollars_4,type="l",xlab="Date",ylab="New Reviews", main="New Reviews of $$$$ Restaurants")
#highly seasonal
```



```{r chunk11, echo=FALSE, warning=FALSE}
df_dollars_1_dlog = as.data.frame(diff(log(df_dollars_1$coredata.df_d_1.)))
df_dollars_2_dlog = as.data.frame(diff(log(df_dollars_2$coredata.df_d_2.)))
df_dollars_3_dlog = as.data.frame(diff(log(df_dollars_3$coredata.df_d_3.)))
df_dollars_4_dlog = as.data.frame(diff(log(df_dollars_4$coredata.df_d_4.)))


#ts of dlogs
ts_dollar_1=ts(df_dollars_1$coredata.df_d_1.,start=c(2006,2),freq=12)
ts_dollar_2=ts(df_dollars_2$coredata.df_d_2.,start=c(2006,2),freq=12)
ts_dollar_3=ts(df_dollars_3$coredata.df_d_3.,start=c(2006,2),freq=12)
ts_dollar_4=ts(df_dollars_4$coredata.df_d_4.,start=c(2006,2),freq=12)


par(mfrow=c(2,2))
plot(ts_dollar_1,type="l",xlab="Date",ylab="New Reviews", main="New Reviews of $ Restaurants")
rect(2007.9166667, -1000, 2009.5, 20000, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)
plot(ts_dollar_2,type="l",xlab="Date",ylab="New Reviews", main="New Reviews of $$ Restaurants")
rect(2007.9166667, -3000, 2009.5, 40000, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)
plot(ts_dollar_3,type="l",xlab="Date",ylab="New Reviews", main="New Reviews of $$$ Restaurants")
rect(2007.9166667, -1000, 2009.5, 7000, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)
plot(ts_dollar_4,type="l",xlab="Date",ylab="New Reviews", main="New Reviews of $$$$ Restaurants")
rect(2007.9166667, -1000, 2009.5, 2000, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)

# 
# dollars_1_dlog=diff(log(ts_dollar_1))
# dollars_2_dlog=diff(log(ts_dollar_2))
# dollars_3_dlog=diff(log(ts_dollar_3))
# dollars_4_dlog=diff(log(ts_dollar_4))
# 
# par(mfrow=c(2,2))
# plot(dollars_1_dlog,xlab="Date",ylab="New Reviews Growth Rate", main="New Reviews Growth Rate of $ Restaurants")
# plot(dollars_2_dlog,xlab="Date",ylab="New Reviews Growth Rate", main="New Reviews Growth Rate of $$ Restaurants")
# plot(dollars_3_dlog,xlab="Date",ylab="New Reviews Growth Rate", main="New Reviews Growth Rate of $$$ Restaurants")
# plot(dollars_4_dlog,xlab="Date",ylab="New Reviews Growth Rate", main="New Reviews Growth Rate of $$$$ Restaurants")

#we can see highly seasonal

# lm_dollars_recession_dummy = recession_dummy_dollars_m[2:length(recession_dummy_dollars_m)]

```

There is a strong and obvious trend along with seasonality. This is something that should be wiped out as much as possible to see the true effect of the recession.


```{r chunk12, echo=FALSE, warning=FALSE}

#seasonally adjust data and remove trend

par(mfrow=c(2,2))

#levels
tslm_d1 = tslm(ts_dollar_1~trend+season)
#summary(tslm_d1)
tslm_d1_resid = resid(tslm_d1)
plot(tslm_d1_resid,xlab="Date",ylab="New Reviews",main="Adjusted Reviews, $")
rect(2007.9166667, -3000, 2009.5, 3000, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)
lm_d1_adj = lm(tslm_d1_resid~recession_dummy_dollars_m)
summary(lm_d1_adj)


tslm_d2 = tslm(ts_dollar_2~trend+season)
#summary(tslm_d1)
tslm_d2_resid = resid(tslm_d2)
plot(tslm_d2_resid,xlab="Date",ylab="New Reviews",main="Adjusted Reviews, $$")
rect(2007.9166667, -7000, 2009.5, 7000, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)
lm_d2_adj = lm(tslm_d2_resid~recession_dummy_dollars_m)
summary(lm_d2_adj)


tslm_d3 = tslm(ts_dollar_3~trend+season)
#summary(tslm_d1)
tslm_d3_resid = resid(tslm_d3)
plot(tslm_d3_resid,xlab="Date",ylab="New Reviews",main="Adjusted Reviews, $$$")
rect(2007.9166667, -500, 2009.5, 500, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)
lm_d3_adj = lm(tslm_d3_resid~recession_dummy_dollars_m)
summary(lm_d3_adj)


tslm_d4 = tslm(ts_dollar_4~trend+season)
#summary(tslm_d1)
tslm_d4_resid = resid(tslm_d4)
plot(tslm_d4_resid,xlab="Date",ylab="New Reviews",main="Adjusted Reviews, $$$$")
rect(2007.9166667, -500, 2009.5, 500, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)
lm_d4_adj = lm(tslm_d4_resid~recession_dummy_dollars_m)
summary(lm_d4_adj)

#descriptive_stats(lm_d1_adj, "New Reviews ($) on Recession")
#descriptive_stats(lm_d2_adj, "New Reviews ($$) on Recession")
#descriptive_stats(lm_d3_adj, "New Reviews ($$$) on Recession")
#descriptive_stats(lm_d4_adj, "New Reviews ($$$$) on Recession")

```

The regression results and error descriptive statistics do not look too promising, but could be leading in the right direction.

Let's try to make it better.

#Making a Better Model

By controlling for the aggegated number of reviews between dollar signs, we can eliminate the effect that a drop in aggregated reviews (again, by dollar signs) can potentially have.

```{r chunk13, echo=FALSE, warning=FALSE}
#levels
lm_d1_adj_rev = lm(tslm_d1_resid~recession_dummy_dollars_m+df_rev_count$coredata.df_rev_m.)
summary(lm_d1_adj_rev)
#descriptive_stats(lm_d1_adj_rev, "New Reviews ($) on Recession and New Reviews (All)")

lm_d2_adj_rev = lm(tslm_d2_resid~recession_dummy_dollars_m+df_rev_count$coredata.df_rev_m.)
summary(lm_d2_adj_rev)
#descriptive_stats(lm_d2_adj_rev, "New Reviews ($$) on Recession and New Reviews (All)")

lm_d3_adj_rev = lm(tslm_d3_resid~recession_dummy_dollars_m+df_rev_count$coredata.df_rev_m.)
summary(lm_d3_adj_rev)
#descriptive_stats(lm_d3_adj_rev, "New Reviews ($$$) on Recession and New Reviews (All)")

lm_d4_adj_rev = lm(tslm_d4_resid~recession_dummy_dollars_m+df_rev_count$coredata.df_rev_m.)
summary(lm_d4_adj_rev)
#descriptive_stats(lm_d4_adj_rev, "New Reviews ($$$$) on Recession and New Reviews (All)")

```

Adding new reviews in doesn't help much, but what about if we detrend and seasonally adjust?

```{r chunk14, echo=FALSE, warning=FALSE}
#detrend review counts

ts_rev_count=ts(df_rev_count$coredata.df_rev_m.,start=c(2006,2),freq=12)
ts_rev_count_tslm = tslm(ts_rev_count~trend+season)
summary(ts_rev_count_tslm)
ts_rev_count_adj = resid(ts_rev_count_tslm,ylab="New Reviews", main="Adjusted Reviews, Total")
plot(ts_rev_count_adj)
rect(2007.9166667, -9000, 2009.5, 9000, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)

lm_d1_adj_rev_adj = lm(tslm_d1_resid~recession_dummy_dollars_m+ts_rev_count_adj)
summary(lm_d1_adj_rev_adj)
#descriptive_stats(lm_d1_adj_rev_adj, "New Reviews ($) on Recession and New Reviews Adjusted (All)")

lm_d2_adj_rev_adj = lm(tslm_d2_resid~recession_dummy_dollars_m+ts_rev_count_adj)
summary(lm_d2_adj_rev_adj)
#descriptive_stats(lm_d2_adj_rev_adj, "New Reviews ($$) on Recession and New Reviews Adjusted (All)")

lm_d3_adj_rev_adj = lm(tslm_d3_resid~recession_dummy_dollars_m+ts_rev_count_adj)
summary(lm_d3_adj_rev_adj)
#descriptive_stats(lm_d3_adj_rev_adj, "New Reviews ($$$) on Recession and New Reviews Adjusted (All)")

lm_d4_adj_rev_adj = lm(tslm_d4_resid~recession_dummy_dollars_m+ts_rev_count_adj)
summary(lm_d4_adj_rev_adj)
#descriptive_stats(lm_d4_adj_rev_adj, "New Reviews ($$$$) on Recession and New Reviews Adjusted (All)")

vif(lm_d1_adj_rev_adj)
vif(lm_d2_adj_rev_adj)
vif(lm_d3_adj_rev_adj)
vif(lm_d4_adj_rev_adj)

```

Although not perfect, using detrended and seasonally adjusted data for the new reviews improves our results drastically. We finally have results that can be trustworthy.

So what do our new results tell us?

We see a decrease in (\$\$\$\$) and (\$\$\$) reviews, but an increase in (\$\$) reviews. Not only do people prefer (\$\$) restaurants over the more expensive (\$\$\$) and (\$\$\$\$) restaurants, it seems that customers who would originally have dined at the more expensive eateries are now choosing the less expensive (\$\$) restaurants. A surprising results is that the least expensive (\$) restaurants do not see a significant change. This could be because inexpensive (\$) restauarants are not substitutes for the others while the (\$\$) restaurants can be substitutes for (\$\$\$) and (\$\$\$\$) restaurants.

Since reviews can be modeled when broken down by prices, this gives more promise to the previous review score analysis as long as it is also broken down in the same way.

#Breaking Down Review Scores by Price

After splitting our reviews by prices, a re-examination of review scores is due.

```{r chunk15, echo=FALSE, include=FALSE, message=FALSE}

stars_by_restaurant_dollars = data.frame(restaurant_visits$date)
stars_by_restaurant_dollars$dollars = get_dollar_signs(restaurant_visits$attributes)
stars_by_restaurant_dollars$stars = restaurant_visits$stars

stars_by_restaurant_dollars=na.omit(stars_by_restaurant_dollars)
stars_by_restaurant_dollars$restaurant_visits.date=as.character(stars_by_restaurant_dollars$restaurant_visits.date)
colnames(stars_by_restaurant_dollars) = c("date","dollars","stars")

dollars_1_star = sqldf("select * from stars_by_restaurant_dollars where dollars=1")
dollars_2_star = sqldf("select * from stars_by_restaurant_dollars where dollars=2")
dollars_3_star = sqldf("select * from stars_by_restaurant_dollars where dollars=3")
dollars_4_star = sqldf("select * from stars_by_restaurant_dollars where dollars=4")

dollars_obd_1_star = sqldf("select date, stars from dollars_1_star order by date")
dollars_obd_2_star = sqldf("select date, stars from dollars_2_star order by date")
dollars_obd_3_star = sqldf("select date, stars from dollars_3_star order by date")
dollars_obd_4_star = sqldf("select date, stars from dollars_4_star order by date")
  
```

```{r chunk16, echo=FALSE, warning=FALSE}

par(mfrow=c(2,2))

dollars_1_star_xts=xts(dollars_obd_1_star$stars, as.Date(dollars_obd_1_star$date,"%Y-%m-%d"))
df_d_1_star = apply.monthly(dollars_1_star_xts, sum)
df_dollars_1_star = data.frame(date=index(df_d_1_star), coredata(df_d_1_star))
df_dollars_1_star$avg = df_dollars_1_star$coredata.df_d_1_star./df_dollars_1$coredata.df_d_1.
plot(df_dollars_1_star$date,df_dollars_1_star$avg,xlab="Date",ylab="Average Star Rating",main="Average Rating for $ Restaurants")
d1_star_lm = lm(df_dollars_1_star$avg~recession_dummy_dollars_m)
summary(d1_star_lm)


dollars_2_star_xts=xts(dollars_obd_2_star$stars, as.Date(dollars_obd_2_star$date,"%Y-%m-%d"))
df_d_2_star = apply.monthly(dollars_2_star_xts, sum)
df_dollars_2_star = data.frame(date=index(df_d_2_star), coredata(df_d_2_star))
df_dollars_2_star$avg = df_dollars_2_star$coredata.df_d_2_star./df_dollars_2$coredata.df_d_2.
plot(df_dollars_2_star$date,df_dollars_2_star$avg,xlab="Date",ylab="Average Star Rating",main="Average Rating for $$ Restaurants")
d2_star_lm = lm(df_dollars_2_star$avg~recession_dummy_dollars_m)
summary(d2_star_lm)


dollars_3_star_xts=xts(dollars_obd_3_star$stars, as.Date(dollars_obd_3_star$date,"%Y-%m-%d"))
df_d_3_star = apply.monthly(dollars_3_star_xts, sum)
df_dollars_3_star = data.frame(date=index(df_d_3_star), coredata(df_d_3_star))
df_dollars_3_star$avg = df_dollars_3_star$coredata.df_d_3_star./df_dollars_3$coredata.df_d_3.
plot(df_dollars_3_star$date,df_dollars_3_star$avg,xlab="Date",ylab="Average Star Rating",main="Average Rating for $$$ Restaurants")
d3_star_lm = lm(df_dollars_3_star$avg~recession_dummy_dollars_m)
summary(d3_star_lm)


dollars_4_star_xts=xts(dollars_obd_4_star$stars, as.Date(dollars_obd_4_star$date,"%Y-%m-%d"))
df_d_4_star = apply.monthly(dollars_4_star_xts, sum)
df_dollars_4_star = data.frame(date=index(df_d_4_star), coredata(df_d_4_star))
df_dollars_4_star$avg = df_dollars_4_star$coredata.df_d_4_star./df_dollars_4$coredata.df_d_4.
plot(df_dollars_4_star$date,df_dollars_4_star$avg,xlab="Date",ylab="Average Star Rating",main="Average Rating for $$$$ Restaurants")
d4_star_lm = lm(df_dollars_4_star$avg~recession_dummy_dollars_m)
summary(d4_star_lm)


#descriptive_stats(d1_star_lm, "Average Rating ($) on Recession")
#descriptive_stats(d2_star_lm, "Average Rating ($$) on Recession")
#descriptive_stats(d3_star_lm, "Average Rating ($$$) on Recession")
#descriptive_stats(d4_star_lm, "Average Rating ($$$$) on Recession")

#looks like the stars dropping during a recession was only in 1 and 2 dollar signs restaurants

```

The results look better, but still are not that great. If we are to interpret the regression anyways, however, it looks as if the lower priced restaurants have lower review scores. This could be a sign of many things, such as the substituters from (\$\$\$) and (\$\$\$\$) restaurants having higher expectations or people just want more bang for their buck. The higher priced restaurants do not see a change, which could be due to the type of people who still dine there during recessions. They could be part of the group unaffected by the recession.


#Sentiment Analysis

By doing a sentiment analysis on the text in the reviews, we can see the association with words during the recession and the period of time (of equal length) directly after the recession.

```{r chunk17, echo=FALSE, warning=FALSE}

stopWords = removePunctuation(stopwords('SMART'))


restaurant_reviews_rec = with(restaurant_reviews, restaurant_reviews[(restaurant_reviews$date >= "2007-12" & restaurant_reviews$date <= "2009-06"), ])

restaurant_reviews_norec = with(restaurant_reviews, restaurant_reviews[(restaurant_reviews$date > "2009-06" & restaurant_reviews$date <= "2011-12"), ])

#create corupses
corpus_reviews_rec = buildCorpus(restaurant_reviews_rec$text,0)
dtm_rec = DocumentTermMatrix(corpus_reviews_rec)
tidy_rec = tidy(dtm_rec)


corpus_reviews_norec = buildCorpus(restaurant_reviews_norec$text,0)
dtm_norec = DocumentTermMatrix(corpus_reviews_norec)
tidy_norec = tidy(dtm_norec)
```

Word Clouds:

```{r chunk18, echo=FALSE, warning=FALSE}

par(mfrow=c(1,2))
buildWordCloud(corpus_reviews_rec, "Spectral",8, "test")
buildWordCloud(corpus_reviews_norec, "Spectral",8, "test2")


```

It is hard to tell which word cloud is from the recession and which is from the period after, but the one on the left is from the recession.

Sentiment Visualization:

```{r chunk19, echo=FALSE, warning=FALSE}
#reference: https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html
ap_sentiments <- tidy_rec %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

ap_sentiments

ap_sentiments %>%
  count(document, sentiment, wt = count) %>%
  ungroup() %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(sentiment)


ap_sentiments %>%
  count(sentiment, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 2000) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")

#negative: 158714
#positive: 346991
#percent negative: 31.4%
#cheap: 3rd highest negative word
#expensive: 7th




ap_sentiments <- tidy_norec %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

ap_sentiments

ap_sentiments %>%
  count(document, sentiment, wt = count) %>%
  ungroup() %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(sentiment)

ap_sentiments %>%
  count(sentiment, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 7500) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")

#negative: 733329
#positive: 1731938
#percent negative: 29.75%
#cheap: 5th
#expensive: 8th


```

Again, it is difficult to distinguish.

The Sentiment visualization on the top is from the recession. On closer examination of the sentiments, the following statistics are extracted.

Recession:\newline
negative: 158714\newline
positive: 346991\newline
percent negative: 31.4%\newline
cheap negative word rank: 3rd\newline
expensive negative word rank: 7th\newline

Post-Recession:\newline
negative: 733329\newline
positive: 1731938\newline
percent negative: 29.75%\newline
cheap negative word rank: 5th\newline
expensive negative word rank: 8th\newline

There is a higher percentage of negative sentiment during the recession as well as having the words "cheap" and "expensive" rank higher for negative words. The word "cheap", however is not always used in a negative way, but it is still related to price. This shows that reviews are more concerned with price during the recession compared to the period directly following it. It should be noted that there is a large difference in the sample size of words from the two periods. This leads to a soft conclusion that reviewers are more concerned with prices during the recession, as it follows recessionary thinking.

#Connecting Yelp Reviews with the Restaurant Industry

By using restaurant expenditures, there can be a connection made between the review data and the actual restaurant industry.

But first, lets confirm that GDP and the recession can be linked to restaurant expenditures.

```{r chunk20, echo=FALSE, include=FALSE, message=FALSE}

restaurant_expenditures=read.csv("restaurant_expenditures.csv")
# restaurant_expenditures

```

```{r chunk21, echo=FALSE, warning=FALSE}

#add x axis with dates
rest_exp_dates = seq(as.Date("2005/12/01"), by = "quarter",length.out = 44)

plot(rest_exp_dates,restaurant_expenditures$real_exp,type="l",xlab="Date",ylab="Restaurant Expenditures",main="Real Restaurant Expenditures, Quarterly")

#test_stationary(restaurant_expenditures$real_exp)

rest_real_exp_diff_log = diff(log(restaurant_expenditures$real_exp))

#test_stationary(rest_real_exp_diff_log)


rest_exp_dates_diff = rest_exp_dates[2:length(rest_exp_dates)]
plot(rest_exp_dates_diff,rest_real_exp_diff_log,type="l",xlab="Date",ylab="Restaurant Expenditures Growth Rate",main="Real Restaurant Expenditures Growth Rate, Quarterly")




#create var + granger causality for real exp and gdp
gdp_exp_combined = cbind(rest_real_exp_diff_log,gdp_growth_subset)
select=VARselect(gdp_exp_combined,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_gdp_exp=VAR(gdp_exp_combined,select$select[1])
# plot(vm_gdp_exp$y)
summary(vm_gdp_exp)

grangertest(rest_real_exp_diff_log~gdp_growth_subset[1:length(gdp_growth_subset)],order=select$select[1])
grangertest(gdp_growth_subset[1:length(gdp_growth_subset)]~rest_real_exp_diff_log,order=select$select[1])

rec_exp_diff_log_dummy = add_recession_dummy(rest_exp_dates_diff)
lm_rest_real_exp_diff_log = lm(rest_real_exp_diff_log~rec_exp_diff_log_dummy)
summary(lm_rest_real_exp_diff_log)
#descriptive_stats(lm_rest_real_exp_diff_log, "Restaurant Expenditures on Recession")

```

The assumption that restaurant expenditures are related to the recession and GDP is confirmed. GDP will not be needed in constructing a full model as restaurant expenditures will be used in its place.

By seasonally adjusting the growth rate in new reviews, a Granger causality test can be run between restaurant expenditures and new reviews.

```{r chunk22, echo=FALSE, warning=FALSE}

# rev_exp_combined = cbind(rest_real_exp_diff_log,log_rev_quarter)
# select=VARselect(rev_exp_combined,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
# vm_rev_exp=VAR(rev_exp_combined,select$select[1])
# # plot(vm_rev_exp$y)
# summary(vm_rev_exp)
# 
# grangertest(rest_real_exp_diff_log~log_rev_quarter, order=select$select[1])
# grangertest(log_rev_quarter~rest_real_exp_diff_log, order=select$select[1])
# 
# 
# #try to seasonally adjust
# #leave trend in because the original seasonally adjusted rest exp had a trend

log_rev_quarter_tslm = tslm(log_rev_quarter~ season)
summary(log_rev_quarter_tslm)
log_rev_quarter_adj = resid(log_rev_quarter_tslm)
# plot(log_rev_quarter_adj)


rev_exp_adj_combined = cbind(rest_real_exp_diff_log,log_rev_quarter_adj)
select=VARselect(rev_exp_adj_combined,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_rev_adj_exp=VAR(rev_exp_adj_combined,select$select[1])
# plot(vm_rev_adj_exp$y)
summary(vm_rev_adj_exp)

grangertest(rest_real_exp_diff_log~log_rev_quarter_adj, order=select$select[1])
grangertest(log_rev_quarter_adj~rest_real_exp_diff_log, order=select$select[1])


plot(log_rev_quarter_adj, xlab="Date",ylab="New Review Adjusted Growth Rate",main="New Review Adjusted Growth Rate, Quarterly")
rect(2007.9166667, -1, 2009.5, 1, col=rgb(red=150/255,green=25/255,blue=25/255,alpha=.3), border=NA)
# plot(rest_real_exp_diff_log,type="l")



```

New reviews is Granger caused by restaurant expenditures. This makes sense as reviews are typically made after people visit a restaurant. This shows that the number of reviews can in fact be related to the previous restaurant expenditures.

#Building Full Models

A full model that incorporates all the previous results will be useful to show the real effect of the recession and determine if there is a causal inference that can be made.

```{r chunk23, echo=FALSE, warning=FALSE}
#convert to quarterly
#add in rest exp
#try var model with adding in rest exp

#adjusted num new rev by dollar signs
d1_quarterly = apply.quarterly(as.xts(tslm_d1_resid),FUN=sum)
ts_d1_quarterly = ts(d1_quarterly,start=c(2006,1),freq=4)

d2_quarterly = apply.quarterly(as.xts(tslm_d2_resid),FUN=sum)
ts_d2_quarterly = ts(d2_quarterly,start=c(2006,1),freq=4)

d3_quarterly = apply.quarterly(as.xts(tslm_d3_resid),FUN=sum)
ts_d3_quarterly = ts(d3_quarterly,start=c(2006,1),freq=4)

d4_quarterly = apply.quarterly(as.xts(tslm_d4_resid),FUN=sum)
ts_d4_quarterly = ts(d4_quarterly,start=c(2006,1),freq=4)

#review counts by quarter
rev_count_adj_q = apply.quarterly(as.xts(ts_rev_count_adj),FUN=sum)
ts_rev_count_adj_q = ts(rev_count_adj_q,start=c(2006,1),freq=4)

#dolla dolla stars
d1_star_quarterly = apply.quarterly(xts(df_dollars_1_star$avg,as.Date(df_dollars_1_star$date,"%Y-%m-%d")),FUN=sum)
ts_d1_star_quarterly = ts(d1_star_quarterly,start=c(2006,1),freq=4)

d2_star_quarterly = apply.quarterly(xts(df_dollars_2_star$avg,as.Date(df_dollars_2_star$date,"%Y-%m-%d")),FUN=sum)
ts_d2_star_quarterly = ts(d2_star_quarterly,start=c(2006,1),freq=4)

d3_star_quarterly = apply.quarterly(xts(df_dollars_3_star$avg,as.Date(df_dollars_3_star$date,"%Y-%m-%d")),FUN=sum)
ts_d3_star_quarterly = ts(d3_star_quarterly,start=c(2006,1),freq=4)

d4_quarterly = apply.quarterly(xts(df_dollars_4_star$avg,as.Date(df_dollars_4_star$date,"%Y-%m-%d")),FUN=sum)
ts_d4_star_quarterly = ts(d4_quarterly,start=c(2006,1),freq=4)

#rec quarter
rec_q=add_recession_dummy(index(d1_star_quarterly))

#since in levels, use trend adjusted level of expenditures (detrend)

rest_exp_q = ts(restaurant_expenditures$real_exp,start=c(2006,1),freq=4)
rest_exp_q_tslm = tslm(rest_exp_q~trend)
rest_exp_q_adj = resid(rest_exp_q_tslm)






lm_d1_full = lm(ts_d1_quarterly~rec_q+ts_rev_count_adj_q+ts_d1_star_quarterly+rest_exp_q_adj)
summary(lm_d1_full)
#descriptive_stats(lm_d1_full, "Full Model ($)")

lm_d2_full = lm(ts_d2_quarterly~rec_q+ts_rev_count_adj_q+ts_d2_star_quarterly+rest_exp_q_adj)
summary(lm_d2_full)
#descriptive_stats(lm_d2_full, "Full Model ($$)")

lm_d3_full = lm(ts_d3_quarterly~rec_q+ts_rev_count_adj_q+ts_d3_star_quarterly+rest_exp_q_adj)
summary(lm_d3_full)
#descriptive_stats(lm_d3_full, "Full Model ($$$)")

lm_d4_full = lm(ts_d4_quarterly~rec_q+ts_rev_count_adj_q+ts_d4_star_quarterly+rest_exp_q_adj)
summary(lm_d4_full)
#descriptive_stats(lm_d4_full, "Full Model ($$$$)")


vif(lm_d1_full)
vif(lm_d2_full)
vif(lm_d3_full)
vif(lm_d4_full)

#lots of insignificance and multicollinearity


```

The regression results and error descriptive statistics are actually worse in this model. There is even multicolinearity occuring. The better model is actually in the form of Restaurant Review (dollars) regressed on Recession and Adjusted New Reviews.

Since new reviews is Granger caused by restaurant expenditures, can we find a causal inference through comprehensive VAR models that include the "Better" model, with restaurant expenditures added in?

```{r chunk24, echo=FALSE, warning=FALSE}

#how about a var
#standardize so min value is 1
ts_d1_quarterly_growth = diff(log(ts_d1_quarterly+1-min(ts_d1_quarterly)))
ts_d2_quarterly_growth = diff(log(ts_d2_quarterly+1-min(ts_d2_quarterly)))
ts_d3_quarterly_growth = diff(log(ts_d3_quarterly+1-min(ts_d3_quarterly)))
ts_d4_quarterly_growth = diff(log(ts_d4_quarterly+1-min(ts_d4_quarterly)))

# rec_dummy_rev_growth_q,rest_real_exp_diff_log,log_rev_quarter_adj


combinedFull1 = cbind(ts_d1_quarterly_growth,rec_dummy_rev_growth_q,log_rev_quarter_adj,rest_real_exp_diff_log)
select=VARselect(combinedFull1,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
fullVar1=VAR(combinedFull1,p=select$select[1])
plot(fullVar1$y)
# summary(fullVar1)

combinedFull2 = cbind(ts_d2_quarterly_growth,rec_dummy_rev_growth_q,log_rev_quarter_adj,rest_real_exp_diff_log)
select=VARselect(combinedFull2,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
fullVar2=VAR(combinedFull2,p=select$select[1])
plot(fullVar2$y)
# summary(fullVar2)

combinedFull3 = cbind(ts_d3_quarterly_growth,rec_dummy_rev_growth_q,log_rev_quarter_adj,rest_real_exp_diff_log)
select=VARselect(combinedFull3,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
fullVar3=VAR(combinedFull3,p=select$select[1])
plot(fullVar3$y)
# summary(fullVar3)

combinedFull4 = cbind(ts_d4_quarterly_growth,rec_dummy_rev_growth_q,log_rev_quarter_adj,rest_real_exp_diff_log)
select=VARselect(combinedFull4,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
fullVar4=VAR(combinedFull4,p=select$select[1])
plot(fullVar4$y)
# summary(fullVar4)




```

The results of the VAR models (surpressed) are not very assuring. They don't point to any real causal inferences. More research and work needs to be done to create a truley causal model.

#Conclusion

The most effective model has been using the amount of reviews by dollar signs and regressing them on the recession and seasonally adjusted new reviews. These models showed that there is a drop in reviews for the higher priced (\$\$\$) and (\$\$\$\$) restaurants while there is an increase in (\$\$) restaurants, with (\$) restaurants being unaffected. This means that the (\$\$) restaurants are a substitute for the more expensive restaurants and that the lowest priced (\$) restaurants are not interchangable with the other three categories.

Sentiment analysis on the review text showed that there was a possibility that reviews became more negative and price focused during the recession. However, the differences in sample size and potentially statistically insignificance of the results leave the sentiment analysis without a definite answer.

It was determined that the VAR model was not very effective in showing potential causality.

#Sources

YELP: https://www.yelp.com/dataset_challenge \newline
FRED: https://fred.stlouisfed.org/  \newline
BEA: https://www.bea.gov/iTable/iTable.cfm?reqid=9&step=1&acrdn=2#reqid=9&step=1&isuri=1&904=2004&903=64&906=q&905=2016&910=x&911=0 \newline
Yahoo Finance: https://finance.yahoo.com/quote/YELP?p=YELP \newline


