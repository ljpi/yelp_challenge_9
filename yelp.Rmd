---
title: "Yelpers in The Great Recession"
date: "2/7/2017"
output:
  pdf_document: default
  latex_engine: xelatex
  html_document: default
---

```{r}
library(jsonlite)
library('vars')
library('readxl')
library('Quandl')
library('tseries')
library("forecast")
library("quantmod")
library("xts")
library("tis")
library("moments")
library('stats')
library("strucchange")
library(knitr)
library(rmarkdown)
library(sqldf)
library("hashmap")
library(stringr)
library(fpp)
library(tm)
library(SnowballC)
library(wordcloud)
library(nlme)
```

```{r setup, include=FALSE}
opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
opts_chunk$set(dev = 'pdf')
```

#outline
intro
data set explantions
  yelp data
  fred
analysis
  yelp users time series
  yelp reviews time series
  yelp review stars time series
  fred data for recession analysis
  yelp stock data
conclusion 

#Introduction

Hypothesis: 
yelpers



#Data Sets

yelp
fred
yahoo finance
bea






```{r}
#SETUP
setwd("C:/cygwin64/home/Lester/yelp_challenge_9")

#load data example
#json_file = file("yelp_academic_dataset_checkin.json")
#json_data = jsonlite::stream_in(json_file)
#head(json_data)
#length(json_data$business_id)

load_json = function(filename){
  json_file = file(filename)
  json_data = jsonlite::stream_in(json_file)
  return(json_data)
}

#business = load_json("yelp_academic_dataset_business.json")
#review = load_json("yelp_academic_dataset_review.json")
#checkin = load_json("yelp_academic_dataset_checkin.json")
#tip = load_json("yelp_academic_dataset_tip.json")
#user = load_json("yelp_academic_dataset_user.json")

remove_lists_from_df = function(df){
  i=1
  while(i <= length(df)){
    if(class(df[,i])=="list"){
      df[i] = sapply(df[,i], paste, collapse="|")
    }
    i=i+1
  }
  
  return(df)
}


add_recession_dummy = function(l){
  rec=c()
  for(i in 1:length(l)){
    
    if(l[i]>=as.Date("2007-12-01")&l[i]<=as.Date("2009-07-01")){
      rec=c(rec,1)
    }
    else{
      rec=c(rec,0)
    }
  }
  
  return(rec)
  
}

```

#user data
```{r}
user = load_json("yelp_academic_dataset_user.json")
colnames(user)
subset_users = user[4]
subset_users
users_by_date_all = sqldf("select yelping_since, count(yelping_since) from subset_users group by yelping_since order by yelping_since")


#usethis
users_by_date = with(users_by_date_all, users_by_date_all[(users_by_date_all$yelping_since >= "2006" & users_by_date_all$yelping_since < "2017"), ])

users_by_date_xts=xts(users_by_date$`count(yelping_since)`, as.Date(users_by_date$yelping_since,"%Y-%m-%d"))
ts_m = apply.monthly(users_by_date_xts, sum)

ts_m

# users_by_date=sqldf("delete from users_by_date where yelping_since < '2012-03-02' ")
# head(users_by_date)
#ts_users = as.ts(users_by_date$`count(yelping_since)`, start = c(2004,10,12), end = c(2017,1,20), frequency=1)

users_df = data.frame(date=index(ts_m), coredata(ts_m))
users_df

stock_users = with(users_df, users_df[(users_df$date >= "2012-02-01"), ])

plot(stock_users,type="l")

yelp_stock = get.hist.quote("YELP",end="2017-01-30",quote="AdjClose",compression="m")
yelp_stock

plot(yelp_stock)

test_stationary = function(t){
  print(kpss.test(t))
  print(adf.test(t))
}

test_stationary(ts(stock_users$coredata.ts_m.,start=c(2012,3),freq=12))
test_stationary(yelp_stock)

log_user_growth = as.data.frame(diff(log(stock_users$coredata.ts_m.)))

log_yelp_growth = as.data.frame(diff(log(yelp_stock)))

log_user_growth
log_yelp_growth




ts_users = ts(log_user_growth,start=c(2012,4),freq=12)
test_stationary(ts_users)

ts_yelp = ts(log_yelp_growth,start=c(2012,4),freq=12)
test_stationary(ts_yelp)

plot(ts_users,col="blue")
lines(ts_yelp,col="red")

sp500 = get.hist.quote("^GSPC",quote="AdjClose",compression="m",start=as.Date("2012-03-01","%Y-%m-%d"),end=as.Date("2017-01-30","%Y-%m-%d"))

sp500
test_stationary(sp500)

log_sp500_growth = as.data.frame(diff(log(sp500)))
ts_sp500 = ts(log_sp500_growth,start=c(2012,4),freq=12)
ts_sp500
test_stationary(ts_sp500)

ts_yelp
ts_sp500


plot(ts_users,col="blue")
lines(ts_yelp,col="red")

combined = cbind(ts_yelp,ts_sp500)

select=VARselect(combined,lag.max=12,type=c("const","trend","both","none"),season=NULL,exogen=NULL)

vm=VAR(combined,p=12)

plot(vm$y)

summary(vm)


```

Growth rate of users do not have an effect on the stock value of Yelp - no correlation to stock market. good! This elinates the possibility that effects are due to yelp as a company doing good or bad.

should i examine review scores + users + etc and its relation to s and p500 during recession? or just move on to gdp and recession

#review scores and gdp

```{r}

review = load_json("yelp_academic_dataset_review.json")

#join with restaurant reviews
business = load_json("yelp_academic_dataset_business.json")
business = remove_lists_from_df(business)

#sanity check the states for subsetting into states
sanity_state = sqldf("select state from business group by state")
sanity_state

#us only
restaurants = sqldf("select business_id from business where (categories like '%Restaurants%' and (state = 'AZ' or state = 'IL' or state = 'NC' or state = 'NV' or state = 'NY' or state = 'OH' or state = 'PA' or state = 'SC' or state = 'VT' or state = 'WI'))")

#us only
restaurant_reviews_all = sqldf("select * from review join restaurants on review.business_id = restaurants.business_id")
colnames(restaurant_reviews_all) = c("review_id","user_id","b_id","stars","date","text","useful","funny","cool","type","business_id")

#use this for reviews, start from 2006 for larger sample sizes (<2006 extremely low sample sizes & heterskedasticity)
restaurant_reviews = with(restaurant_reviews_all, restaurant_reviews_all[(restaurant_reviews_all$date >= "2006" & restaurant_reviews_all$date < "2017"), ])


review_date_star = as.data.frame(restaurant_reviews$date)
review_date_star$stars = restaurant_reviews$stars
head(review_date_star)
colnames(review_date_star)=c("date","stars")


review_date_star$date=as.Date(review_date_star$date,"%Y-%m-%d")

#freq
review_counts_by_date = sqldf("select date, count(date) from review_date_star group by date order by date")

head(review_counts_by_date)

plot(review_counts_by_date$date,review_counts_by_date$`count(date)`,type="l")

#convert to monthly
reviews_by_date=xts(review_counts_by_date$`count(date)`, as.Date(review_counts_by_date$date,"%Y-%m-%d"))
ts_rev_m = apply.monthly(reviews_by_date, sum)
ts_rev_count = data.frame(date=index(ts_rev_m), coredata(ts_rev_m))
ts_rev_count

plot(ts_rev_count$date,ts_rev_count$coredata.ts_rev_m.,type="l")


#review growth rates/new user growth rates
log_rev_count=diff(log(ts_rev_count$coredata.ts_rev_m.))
#log_rev_count[1]=NA
log_rev_count = na.omit(log_rev_count)
log_rev_count = ts(log_rev_count,start=c(2006,2),freq=12)

ts_m
log_user_count=diff(log(ts_m[,1]))
log_user_count = na.omit(log_user_count)
log_user_count = ts(log_user_count,start=c(2006,2),freq=12)

plot(log_rev_count,type="l", main="growth rate of user reviews and accounts")
lines(log_user_count[,1],col="red")

#do a var model between growth rate of users revs and accounts

#create var of growth rates
rates_combined = cbind(log_rev_count,log_user_count)
select=VARselect(rates_combined,lag.max=12,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_rates=VAR(rates_combined,select$select[1])
plot(vm_rates$y)
summary(vm_rates)

#do granger causality test
grangertest(log_rev_count~log_user_count,order=select$select[1])
grangertest(log_user_count~log_rev_count,order=select$select[1])

user_rev_gls=gls(log_rev_count~log_user_count)
summary(user_rev_gls)

#stars by month
stars_by_date=xts(review_date_star$stars, as.Date(review_date_star$date,"%Y-%m-%d"))
ts_stars_m = apply.monthly(stars_by_date, sum)
ts_stars = data.frame(date=index(ts_stars_m), coredata(ts_stars_m))


ts_stars$avg = ts_stars$coredata.ts_stars_m./ts_rev_count$coredata.ts_rev_m.
head(ts_stars)

plot(ts_stars$date,ts_stars$avg,type = "l")
#evidence of recession in stars
stars_recession_dummy = add_recession_dummy(ts_stars$date)
stars_avg=ts_stars$avg
stars_gls=gls(stars_avg~stars_recession_dummy)
summary(stars_gls)



test_stationary(ts_stars$avg)
#convert to growth rates
ts_stars_diff_log = as.data.frame(diff(log(ts_stars$avg)))
ts_stars_diff_log$date=ts_stars$date[2:length(ts_stars$date)]

ts_stars_diff_log
colnames(ts_stars_diff_log)=c("avg","date")

test_stationary(ts_stars_diff_log$avg)


stars_recession_diff_log_dummy = add_recession_dummy(ts_stars_diff_log$date)

# stars_reg = lm(ts_stars_diff_log$avg ~ stars_recession_diff_log_dummy)
# summary(stars_reg)

plot(ts_stars_diff_log$date,ts_stars_diff_log$avg,type="l")

stars_diff_log_avg = ts_stars_diff_log$avg
stars_diff_log_gls = gls(stars_diff_log_avg~stars_recession_diff_log_dummy)
summary(stars_diff_log_gls)

```

the growth rates plot shows that even during recession we don't see a dip in either. if there was constant account creation but a dip in reviews, that would mean the recession has an effect on num of reviews

#descriptive stats of reviews and stars

```{r}
sd(ts_rev_count$coredata.ts_rev_m.)
mean(ts_rev_count$coredata.ts_rev_m.)


sd(log_rev_count)
mean(log_rev_count)


sd(ts_stars$avg)
mean(ts_stars$avg)

```

#fred data

```{r}




#real gdp
getSymbols('GDPC96', src = 'FRED')
gdp = GDPC96

plot(gdp)

gdp_growth = na.omit(diff(log(gdp)))
plot(gdp_growth)
gdp_growth_subset = with(gdp_growth, gdp_growth[index(gdp_growth) >= "2006-04-01" & index(gdp_growth) < "2016-12-30", ])
gdp_growth_subset = ts(gdp_growth_subset,start=c(2006,2),frequency = 4)
plot(gdp_growth_subset)

#create quarterly review growth rate
reviews_by_quarter=xts(review_counts_by_date$`count(date)`, as.Date(review_counts_by_date$date,"%Y-%m-%d"))
ts_rev_m_quarter = apply.quarterly(reviews_by_quarter, sum)
ts_rev_quarter = data.frame(date=index(ts_rev_m_quarter), coredata(ts_rev_m_quarter))
ts_rev_quarter
plot(ts_rev_quarter$date,ts_rev_quarter$coredata.ts_rev_m_quarter.,type="l")

log_rev_quarter=diff(log(ts_rev_quarter$coredata.ts_rev_m_quarter.))
log_rev_quarter = na.omit(log_rev_quarter)
log_rev_quarter = ts(log_rev_quarter,start=c(2006,2),freq=4)
plot(log_rev_quarter,type="l")

length(log_rev_quarter)
length(gdp_growth_subset)

#var of gdp and user reviews growth rates
gdp_combined = cbind(log_rev_quarter,gdp_growth_subset)
select=VARselect(gdp_combined,lag.max=12,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_gdp=VAR(gdp_combined,p=12)
plot(vm_gdp$y)
summary(vm_gdp)

#do a var on quarterly gdp growth and quarterly avg review score

recession_dummy_reviews_q = add_recession_dummy(ts_rev_quarter$date)

reg_reviews = lm(ts_rev_quarter$coredata.ts_rev_m_quarter.~recession_dummy_reviews_q)
summary(reg_reviews)
#first reg is misleading



#look at growth rates

rec_dummy_rev_growth_q = recession_dummy_reviews_q[2:length(recession_dummy_reviews_q)]

reg_log_reviews = lm(log_rev_quarter~rec_dummy_rev_growth_q)
summary(reg_log_reviews)

#use gls
log_reviews_gls = gls(log_rev_quarter~rec_dummy_rev_growth_q)
summary(log_reviews_gls)

```




#examine social dynamics during recessions through dollar signs as well as review texts
do people eat at cheaper places? do people care more about overpriced food?
```{r}

####CHANGE THIS SECTION TO USE TSLM#####


#convert $ dollar signs to time series
restaurant_visits = sqldf("select restaurant_reviews.date, business.attributes from restaurant_reviews join business on restaurant_reviews.business_id = business.business_id")

#create a dollar sign extractor
get_dollar_signs=function(s){
  temp=str_extract(s,"RestaurantsPriceRange2(.*?)[0-9]")
  return(substr(temp,nchar(temp),nchar(temp)))
}


restaurant_dollars=data.frame(restaurant_visits$date)
restaurant_dollars$dollars = get_dollar_signs(restaurant_visits$attributes)

restaurant_dollars=na.omit(restaurant_dollars)
class(restaurant_dollars$restaurant_visits.date)
restaurant_dollars$restaurant_visits.date=as.character(restaurant_dollars$restaurant_visits.date)
colnames(restaurant_dollars) = c("date","dollars")

dollars_1 = sqldf("select * from restaurant_dollars where dollars=1")
dollars_2 = sqldf("select * from restaurant_dollars where dollars=2")
dollars_3 = sqldf("select * from restaurant_dollars where dollars=3")
dollars_4 = sqldf("select * from restaurant_dollars where dollars=4")

dollars_gbd_1 = sqldf("select date, count(date) from dollars_1 group by date order by date")
dollars_gbd_2 = sqldf("select date, count(date) from dollars_2 group by date order by date")
dollars_gbd_3 = sqldf("select date, count(date) from dollars_3 group by date order by date")
dollars_gbd_4 = sqldf("select date, count(date) from dollars_4 group by date order by date")

dollars_1_xts=xts(dollars_gbd_1$`count(date)`, as.Date(dollars_gbd_1$date,"%Y-%m-%d"))
df_d_1 = apply.monthly(dollars_1_xts, sum)
df_dollars_1 = data.frame(date=index(df_d_1), coredata(df_d_1))
df_dollars_1

dollars_2_xts=xts(dollars_gbd_2$`count(date)`, as.Date(dollars_gbd_2$date,"%Y-%m-%d"))
df_d_2 = apply.monthly(dollars_2_xts, sum)
df_dollars_2 = data.frame(date=index(df_d_2), coredata(df_d_2))
df_dollars_2

dollars_3_xts=xts(dollars_gbd_3$`count(date)`, as.Date(dollars_gbd_3$date,"%Y-%m-%d"))
df_d_3 = apply.monthly(dollars_3_xts, sum)
df_dollars_3 = data.frame(date=index(df_d_3), coredata(df_d_3))
df_dollars_3

dollars_4_xts=xts(dollars_gbd_4$`count(date)`, as.Date(dollars_gbd_4$date,"%Y-%m-%d"))
df_d_4 = apply.monthly(dollars_4_xts, sum)
df_dollars_4 = data.frame(date=index(df_d_4), coredata(df_d_4))
df_dollars_4


recession_dummy_dollars_m = add_recession_dummy(df_dollars_1$date)


plot(df_dollars_1,type="l")
plot(df_dollars_2,type="l")
plot(df_dollars_3,type="l")
plot(df_dollars_4,type="l")


df_dollars_1_dlog = as.data.frame(diff(log(df_dollars_1$coredata.df_d_1.)))
df_dollars_2_dlog = as.data.frame(diff(log(df_dollars_2$coredata.df_d_2.)))
df_dollars_3_dlog = as.data.frame(diff(log(df_dollars_3$coredata.df_d_3.)))
df_dollars_4_dlog = as.data.frame(diff(log(df_dollars_4$coredata.df_d_4.)))


plot(df_dollars_1$date[2:length(df_dollars_1$date)],df_dollars_1_dlog$`diff(log(df_dollars_1$coredata.df_d_1.))`,type="l")
plot(df_dollars_2$date[2:length(df_dollars_2$date)],df_dollars_2_dlog$`diff(log(df_dollars_2$coredata.df_d_2.))`,type="l")
plot(df_dollars_3$date[2:length(df_dollars_3$date)],df_dollars_3_dlog$`diff(log(df_dollars_3$coredata.df_d_3.))`,type="l")
plot(df_dollars_4$date[2:length(df_dollars_4$date)],df_dollars_4_dlog$`diff(log(df_dollars_4$coredata.df_d_4.))`,type="l")



#seasonally adjust the data

ts_dollar_1=ts(df_dollars_1$coredata.df_d_1.,start=c(2005,2),freq=12)
decompose_ts_dollar_1=decompose(ts_dollar_1,"multiplicative")
dollar_1_adj = ts_dollar_1/decompose_ts_dollar_1$seasonal
plot(dollar_1_adj)

ts_dollar_2=ts(df_dollars_2$coredata.df_d_2.,start=c(2005,2),freq=12)
decompose_ts_dollar_2=decompose(ts_dollar_2,"multiplicative")
dollar_2_adj = ts_dollar_2/decompose_ts_dollar_2$seasonal
plot(dollar_2_adj)

ts_dollar_3=ts(df_dollars_3$coredata.df_d_3.,start=c(2005,2),freq=12)
decompose_ts_dollar_3=decompose(ts_dollar_3,"multiplicative")
dollar_3_adj = ts_dollar_3/decompose_ts_dollar_3$seasonal
plot(dollar_3_adj)

ts_dollar_4=ts(df_dollars_4$coredata.df_d_4.,start=c(2005,2),freq=12)
decompose_ts_dollar_4=decompose(ts_dollar_4,"multiplicative")
dollar_4_adj = ts_dollar_4/decompose_ts_dollar_4$seasonal
plot(dollar_4_adj)

dollars_1_adj_dlog=diff(log(dollar_1_adj))
dollars_2_adj_dlog=diff(log(dollar_2_adj))
dollars_3_adj_dlog=diff(log(dollar_3_adj))
dollars_4_adj_dlog=diff(log(dollar_4_adj))

plot(dollars_1_adj_dlog)
plot(dollars_2_adj_dlog)
plot(dollars_3_adj_dlog)
plot(dollars_4_adj_dlog)

gls_dollars_recession_dummy = recession_dummy_dollars_m[2:length(recession_dummy_dollars_m)]

gls_log_d1 = gls(dollars_1_adj_dlog~gls_dollars_recession_dummy)
summary(gls_log_d1)
gls_log_d2 = gls(dollars_2_adj_dlog~gls_dollars_recession_dummy)
summary(gls_log_d2)
gls_log_d3 = gls(dollars_3_adj_dlog~gls_dollars_recession_dummy)
summary(gls_log_d3)
gls_log_d4 = gls(dollars_4_adj_dlog~gls_dollars_recession_dummy)
summary(gls_log_d4)

####CHANGE THIS SECTION TO USE TSLM#####



```



#comment out for now, cuz takes too long to process


<!-- analyze word usages -->
<!-- ```{r} -->

<!-- buildCorpus = function(data, stem){ -->
<!--   corpus = Corpus(VectorSource(data)) -->
<!--   corpus = tm_map(corpus, content_transformer(tolower)) -->
<!--   corpus = tm_map(corpus, PlainTextDocument) -->
<!--   corpus = tm_map(corpus, removePunctuation) -->
<!--   corpus = tm_map(corpus, removeWords, stopWords) -->
<!--   if(stem==1) corpus = tm_map(corpus, stemDocument) -->
<!--   return(corpus) -->
<!-- } -->

<!-- buildWordCloud = function(corpus, pal,val, name){ -->
<!--   wordcloud(corpus, max.words = 75, random.order = FALSE, colors=brewer.pal(val,pal), main = name) -->
<!-- } -->
<!-- stopWords = removePunctuation(stopwords('SMART')) -->

<!-- head(restaurant_reviews) -->
<!-- restaurant_reviews_rec = with(restaurant_reviews, restaurant_reviews[(restaurant_reviews$date >= "2007-12" & restaurant_reviews$date <= "2009-06"), ]) -->

<!-- restaurant_reviews_norec = with(restaurant_reviews, restaurant_reviews[(restaurant_reviews$date > "2009-06" & restaurant_reviews$date <= "2011-12"), ]) -->

<!-- #create corupses -->
<!-- corpus_reviews_rec = buildCorpus(restaurant_reviews_rec$text,0) -->
<!-- #corpus_reviews_norec = buildCorpus(restaurant_reviews_norec$text,0) -->



<!-- #buildWordCloud(corpus_reviews_rec, "Spectral",8,0, "test") -->
<!-- #buildWordCloud(corpus_reviews_norec, "Spectral",8,0, "test2") -->


<!-- ``` -->


```{r}

```


#connect with restaurants

```{r}

restaurant_expenditures=read.csv("restaurant_expenditures.csv")
restaurant_expenditures

#add x axis with dates
rest_exp_dates = seq(as.Date("2005/12/01"), by = "quarter",length.out = 44)

plot(rest_exp_dates,restaurant_expenditures$real_exp,type="l")

test_stationary(restaurant_expenditures$real_exp)

rest_real_exp_diff_log = diff(log(restaurant_expenditures$real_exp))

test_stationary(rest_real_exp_diff_log)


rest_exp_dates_diff = rest_exp_dates[2:length(rest_exp_dates)]
plot(rest_exp_dates_diff,rest_real_exp_diff_log,type="l")

#STILL NOT STATIONARY WTF?!



#create var + granger causality for real exp and gdp
gdp_exp_combined = cbind(rest_real_exp_diff_log,gdp_growth_subset)
select=VARselect(gdp_exp_combined,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_gdp_exp=VAR(gdp_exp_combined,select$select[1])
plot(vm_gdp_exp$y)
summary(vm_gdp_exp)

grangertest(rest_real_exp_diff_log~gdp_growth_subset[1:length(gdp_growth_subset)],order=select$select[1])
grangertest(gdp_growth_subset[1:length(gdp_growth_subset)]~rest_real_exp_diff_log,order=select$select[1])

rec_exp_diff_log_dummy = add_recession_dummy(rest_exp_dates_diff)
gls_rest_real_exp_diff_log = gls(rest_real_exp_diff_log~rec_exp_diff_log_dummy)
summary(gls_rest_real_exp_diff_log)

```

yes gdp granger causes restaurant expenditures and recession dummy



#connect reviews with restaurants


```{r}

rev_exp_combined = cbind(rest_real_exp_diff_log,log_rev_quarter)
select=VARselect(rev_exp_combined,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_rev_exp=VAR(rev_exp_combined,select$select[1])
plot(vm_rev_exp$y)
summary(vm_rev_exp)

grangertest(rest_real_exp_diff_log~log_rev_quarter, order=select$select[1])
grangertest(log_rev_quarter~rest_real_exp_diff_log, order=select$select[1])

#looks like log_rev_quarter is granger caused by rest_real_exp_diff_log
log_rev_exp_gls = gls(log_rev_quarter~rec_dummy_rev_growth_q+rest_real_exp_diff_log)
summary(log_rev_exp_gls)
plot(log_rev_exp_gls$fitted,type="l")
plot(log_rev_quarter)

temp = tslm(log_rev_quarter~trend+season)
seasonaly_adj_log_rev=temp$residuals

reg_seasonaly_adj_log_rev = gls(seasonaly_adj_log_rev~rec_dummy_rev_growth_q)
summary(reg_seasonaly_adj_log_rev)




#IS RECESSION AN IV?!


#two stage regression
stage1 = gls(rest_real_exp_diff_log~rec_dummy_rev_growth_q)
summary(stage1)
stage1_resid = resid(stage1)

stage2=gls(seasonaly_adj_log_rev~stage1_resid)
summary(stage2)

#what am i doing....?


# #no granger causality
# 
# grangertest(log_rev_quarter~gdp_growth_subset)
# 
# grangertest(log_rev_quarter~rec_dummy_rev_growth_q)
# 
# log_rev_exp_gls = gls(log_rev_quarter~rec_dummy_rev_growth_q+rest_real_exp_diff_log)
# summary(log_rev_exp_gls)
# 
# testing = tslm(log_rev_quarter~trend+season)
# summary(testing)
# 
# temp=testing$residuals
# 
# reg_Test = gls(temp~rec_dummy_rev_growth_q+rest_real_exp_diff_log)
# summary(reg_Test)
# 
# rev_exp_combined = cbind(rest_real_exp_diff_log,temp)
# select=VARselect(rev_exp_combined,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
# vm_rev_exp=VAR(rev_exp_combined,select$select[1])
# plot(vm_rev_exp$y)
# summary(vm_rev_exp)
# 
# grangertest(rest_real_exp_diff_log~temp,order=select$select[1])
# grangertest(temp~rest_real_exp_diff_log,order=select$select[1])
# 
# 
# plot(vm_rev_exp$y[,1])
# lines(vm_rev_exp$y[,2],col="red")


```



<!-- #seasonally adjust log reviews by quarter -->

<!-- ```{r} -->
<!-- decompose_log_rev_quarter=decompose(log_rev_quarter,"additive") -->
<!-- log_rev_quarter_adj=log_rev_quarter-decompose_log_rev_quarter$seasonal -->
<!-- plot(log_rev_quarter_adj) -->


<!-- rev_adj_exp_combined = cbind(rest_real_exp_diff_log,log_rev_quarter_adj) -->
<!-- select=VARselect(rev_adj_exp_combined,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL) -->
<!-- vm_rev_adj_exp=VAR(rev_adj_exp_combined,select$select[1]) -->
<!-- plot(vm_rev_adj_exp$y) -->
<!-- summary(vm_rev_adj_exp) -->


<!-- gls_rev_adj=gls(log_rev_quarter_adj~rec_dummy_rev_growth_q) -->
<!-- summary(gls_rev_adj) -->

<!-- log_rev_gls = gls(log_rev_quarter~rec_dummy_rev_growth_q+rest_real_exp_diff_log) -->
<!-- summary(log_rev_gls) -->


<!-- log_rev_exp_gls = gls(log_rev_quarter~rec_dummy_rev_growth_q+rest_real_exp_diff_log+rest_real_exp_diff_log) -->
<!-- summary(log_rev_exp_gls) -->
<!-- ``` -->

yelpers are not representative of overall restaurant goers, but are still effected by the recession
















old stuff



<!-- #Exploring the Data -->


<!-- ```{r, echo=FALSE} -->
<!-- business = remove_lists_from_df(business) -->

<!-- business_us = sqldf("select * from business where (state = 'AZ' or state = 'IL'  -->
<!--                     or state = 'NC' or state = 'NV' or state = 'NY' or state = 'OH'  -->
<!--                     or state = 'PA' or state = 'SC' or state = 'VT' or state = 'WI')") -->

<!-- restaurants = sqldf("select * from business where categories like '%Restaurants%'") -->
<!-- restaurants_open = sqldf("select * from restaurants where is_open=1") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #number of states -->
<!-- length(sqldf("select state from restaurants_open group by state")[,1]) -->
<!-- #number of cities -->
<!-- length(sqldf("select city, state from restaurants_open group by city")[,1]) -->
<!-- ``` -->

<!-- After examining the states and cities, there are some omissions, such as Los Angeles and New York City, that could have been extremely interesting if included. Nevertheless, let's see how the distribution of cities look. -->

<!-- ```{r} -->
<!-- city_counts = sqldf("select city, state, count(business_id) as count from restaurants_open group by city") -->

<!-- #ones, 100s, 200s, ..., 1000+ -->
<!-- buckets = c(0,0,0,0,0,0,0,0,0,0,0) -->

<!-- i=1 -->
<!-- while(i <= length(city_counts[,1])){ -->
<!--   temp_count = city_counts$count[i] -->
<!--   if(temp_count<100){ -->
<!--     buckets[1]=buckets[1]+1 -->
<!--   } -->
<!--   else if(temp_count>=100 & temp_count < 200){ -->
<!--     buckets[2]=buckets[2]+1 -->
<!--   } -->
<!--   else if(temp_count>=200 & temp_count < 300){ -->
<!--     buckets[3]=buckets[3]+1 -->
<!--   } -->
<!--   else if(temp_count>=300 & temp_count < 400){ -->
<!--     buckets[4]=buckets[4]+1 -->
<!--   } -->
<!--   else if(temp_count>=400 & temp_count < 500){ -->
<!--     buckets[5]=buckets[5]+1 -->
<!--   } -->
<!--   else if(temp_count>=500 & temp_count < 600){ -->
<!--     buckets[6]=buckets[6]+1 -->
<!--   } -->
<!--   else if(temp_count>=600 & temp_count < 700){ -->
<!--     buckets[7]=buckets[7]+1 -->
<!--   } -->
<!--   else if(temp_count>=700 & temp_count < 800){ -->
<!--     buckets[8]=buckets[8]+1 -->
<!--   } -->
<!--   else if(temp_count>=800 & temp_count < 900){ -->
<!--     buckets[9]=buckets[9]+1 -->
<!--   } -->
<!--   else if(temp_count>=900 & temp_count < 1000){ -->
<!--     buckets[10]=buckets[10]+1 -->
<!--   } -->
<!--   else if(temp_count>=1000){ -->
<!--     buckets[11]=buckets[11]+1 -->
<!--   } -->
<!--   else{ -->
<!--     print(paste(c("ERROR at index: ",i))) -->
<!--   } -->
<!--   i=i+1 -->
<!-- } -->

<!-- plot(buckets) -->

<!-- ``` -->
<!-- ```{r} -->

<!-- mean(city_counts$count) -->
<!-- sd(city_counts$count) -->
<!-- quantile(city_counts$count) -->

<!-- ``` -->

<!-- Looks like most of the cities fall into the first bucket of less than 100 businesses. However, we want to narrow our search to just large cities, so let's remove that bucket. -->

<!-- ```{r} -->
<!-- city_counts2 = sqldf("select * from city_counts where count >100") -->
<!-- buckets2 = buckets[2:length(buckets)] -->
<!-- plot(buckets2) -->
<!-- ``` -->

<!-- ```{r} -->

<!-- mean(city_counts2$count) -->
<!-- sd(city_counts2$count) -->
<!-- quantile(city_counts2$count) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- city_counts3 = sqldf("select * from city_counts where count >100 and count <1000") -->
<!-- mean(city_counts3$count) -->
<!-- sd(city_counts3$count) -->
<!-- quantile(city_counts3$count) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #subset by >245 obs -->
<!-- large_cities = sqldf("select city, state, count(business_id) as count from restaurants_open group by city having count > 245") -->
<!-- large_cities -->
<!-- ``` -->

<!-- Let's take a look at demographics for our potential cities: -->

<!-- ```{r} -->

<!-- demographics = read.csv("demographics.csv",stringsAsFactors = FALSE) -->
<!-- sqldf("select city, pop_by_race_2015 from demographics order by pop_by_race_2015") -->
<!-- ``` -->

<!-- We can see that Champaign definately doesn't meet our criteria for city size, so I'll drop it. Althought Tempe isn't as large as some of the other cities, I will make a judgement call and still group it as a upper mid to large sized city. -->

<!-- ```{r} -->

<!-- #drop champaign -->
<!-- demographics = demographics[-1,] -->
<!-- demographics -->
<!-- ``` -->

<!-- ```{r} -->

<!-- #find all occurances of restaurant types in each location -->

<!-- cities_usethis = sqldf("select city from demographics") -->
<!-- restaurants_usethis = sqldf("select * from restaurants_open where (city='Chandler' or -->
<!--                             city='Charlotte' or city='Cleveland' or city='Gilbert' -->
<!--                             or city='Glendale' or city='Henderson' or city='Las Vegas' -->
<!--                             or city='Madison' or city='Mesa' or city='Phoenix' -->
<!--                             or city='Pittsburgh' or city='Scottsdale' or city='Tempe')") -->

<!-- get_city=function(df,i){ -->
<!--   return(df$city[i]) -->
<!-- } -->


<!-- create_hashmap = function(){ -->
<!--   key="type" -->
<!--   value=0 -->
<!--   temp = hashmap(key,value) -->
<!--   temp$erase(key) -->
<!--   return(temp) -->
<!-- } -->

<!-- hmap_chandler = create_hashmap() -->
<!-- hmap_charlotte = create_hashmap() -->
<!-- hmap_cleveland = create_hashmap() -->
<!-- hmap_gilbert = create_hashmap() -->
<!-- hmap_glendale = create_hashmap() -->
<!-- hmap_henderson = create_hashmap() -->
<!-- hmap_lasvegas = create_hashmap() -->
<!-- hmap_madison = create_hashmap() -->
<!-- hmap_mesa = create_hashmap() -->
<!-- hmap_phoenix = create_hashmap() -->
<!-- hmap_pittsburgh = create_hashmap() -->
<!-- hmap_scottsdale = create_hashmap() -->
<!-- hmap_tempe = create_hashmap() -->

<!-- #takes string, delim by pipe |, counts occurances, updates hmap -->
<!-- update_hashmap = function(hmap,s){ -->
<!--   vec=unlist(strsplit(s,split="\\|")) -->
<!--   len = length(vec) -->
<!--   i = 1 -->
<!--   while(i<=len){ -->

<!--     if(hmap$has_key(vec[i])){ -->
<!--       temp = hmap$find(vec[i]) -->
<!--       temp = temp+1 -->
<!--       hmap$insert(vec[i],temp) -->
<!--     } -->
<!--     else{  -->
<!--       hmap$insert(vec[i],1) -->
<!--     } -->
<!--     i=i+1 -->
<!--   } -->
<!--   return(hmap) -->
<!-- } -->

<!-- i = 1 -->
<!-- while(i <= length(restaurants_usethis$business_id)){ -->
<!--   city=get_city(restaurants_usethis,i) -->
<!--   cat = restaurants_usethis$categories[i] -->
<!--   if(city=="Chandler"){ -->
<!--     hmap_chandler=update_hashmap(hmap_chandler,cat) -->
<!--   } -->
<!--   else if(city=="Charlotte"){ -->
<!--     hmap_charlotte=update_hashmap(hmap_charlotte,cat) -->
<!--   } -->
<!--   else if(city=="Cleveland"){ -->
<!--     hmap_cleveland=update_hashmap(hmap_cleveland,cat) -->
<!--   } -->
<!--   else if(city=="Gilbert"){ -->
<!--     hmap_gilbert=update_hashmap(hmap_gilbert,cat) -->
<!--   } -->
<!--   else if(city=="Glendale"){ -->
<!--     hmap_glendale=update_hashmap(hmap_glendale,cat) -->
<!--   } -->
<!--   else if(city=="Henderson"){ -->
<!--     hmap_henderson=update_hashmap(hmap_henderson,cat) -->
<!--   } -->
<!--   else if(city=="Las Vegas"){ -->
<!--     hmap_lasvegas=update_hashmap(hmap_lasvegas,cat) -->
<!--   } -->
<!--   else if(city=="Madison"){ -->
<!--     hmap_madison=update_hashmap(hmap_madison,cat) -->
<!--   } -->
<!--   else if(city=="Mesa"){ -->
<!--     hmap_mesa=update_hashmap(hmap_mesa,cat) -->
<!--   } -->
<!--   else if(city=="Phoenix"){ -->
<!--     hmap_phoenix=update_hashmap(hmap_phoenix,cat) -->
<!--   } -->
<!--   else if(city=="Pittsburgh"){ -->
<!--     hmap_pittsburgh=update_hashmap(hmap_pittsburgh,cat) -->
<!--   } -->
<!--   else if(city=="Scottsdale"){ -->
<!--     hmap_scottsdale=update_hashmap(hmap_scottsdale,cat) -->
<!--   } -->
<!--   else if(city=="Tempe"){ -->
<!--     hmap_tempe=update_hashmap(hmap_tempe,cat) -->
<!--   } -->
<!--   else{ -->
<!--     print(paste(c("ERROR at index: ",i))) -->
<!--   } -->


<!--   i=i+1 -->
<!-- } -->



<!-- ``` -->

<!-- ```{r} -->

<!-- hmap_vec = c(hmap_chandler,hmap_charlotte,hmap_cleveland,hmap_gilbert, -->
<!--              hmap_glendale,hmap_henderson,hmap_lasvegas,hmap_madison, -->
<!--              hmap_mesa,hmap_phoenix,hmap_pittsburgh,hmap_scottsdale, -->
<!--              hmap_tempe) -->
<!-- hmap_vec -->

<!-- category_demographics=function(hmap_vec,s,demographics){ -->
<!--   #values_vec = c(hmap_vec[1]$find(s),hmap_vec[2]$find(s), -->
<!--   #               hmap_vec[3]$find(s),hmap_vec[4]$find(s), -->
<!--   #               hmap_vec[5]$find(s),hmap_vec[6]$find(s), -->
<!--   #               hmap_vec[7]$find(s),hmap_vec[8]$find(s), -->
<!--   #               hmap_vec[9]$find(s),hmap_vec[10]$find(s), -->
<!--   #               hmap_vec[11]$find(s),hmap_vec[12]$find(s), -->
<!--   #               hmap_vec[13]$find(s),hmap_vec[14]$find(s)) -->
<!--   values_vec=c() -->
<!--   for(i in 1:length(hmap_vec)){ -->
<!--     values_vec = c(values_vec,hmap_vec[[i]]$find(s)) -->
<!--   } -->

<!--   return(values_vec) -->
<!-- } -->

<!-- test = category_demographics(hmap_vec,"Chinese",demographics) -->
<!-- test_reg = lm(test~demographics$asian) -->
<!-- summary(test_reg) -->
<!-- plot(demographics$asian,test) -->
<!-- lines(demographics$asian,test_reg$fitted.values) -->

<!-- #need to factor in %s -->
<!-- asian_percent = demographics$asian/demographics$pop_by_race_2015 -->
<!-- asian_percent -->

<!-- restaurants_usethis -->
<!-- cities_usethis -->
<!-- large_cities -->

<!-- city_restaurant_counts=sqldf("select large_cities.city, count from (large_cities join cities_usethis on large_cities.city=cities_usethis.city)") -->
<!-- city_restaurant_counts -->

<!-- restaurant_chinese_percent=test/city_restaurant_counts$count -->
<!-- restaurant_chinese_percent -->
<!-- percent_reg_test = lm(restaurant_chinese_percent~asian_percent) -->
<!-- summary(percent_reg_test) -->
<!-- percent_reg_test2 = lm(restaurant_chinese_percent~asian_percent+demographics$income_index+demographics$pop_growth) -->
<!-- summary(percent_reg_test2) -->
<!-- ``` -->


<!-- #analyze reviews -->

<!-- ```{r} -->
<!-- #subset reviews to match city data -->
<!-- reviews_usethis=sqldf("select review.stars, date, text, categories, city from review join restaurants_usethis on review.business_id=restaurants_usethis.business_id") -->

<!-- ``` -->




sources:
fred
yelp
https://www.bea.gov/iTable/iTable.cfm?reqid=9&step=1&acrdn=2#reqid=9&step=1&isuri=1&904=2004&903=64&906=q&905=2016&910=x&911=0


