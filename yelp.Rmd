---
title: "Yelpers in The Great Recession"
date: "2/7/2017"
output:
  pdf_document: default
  latex_engine: xelatex
  html_document: default
---

```{r, include=FALSE}
library(jsonlite)
library('vars')
library('readxl')
library('Quandl')
library('tseries')
library("forecast")
library("quantmod")
library("xts")
library("tis")
library("moments")
library('stats')
library("strucchange")
library(knitr)
library(rmarkdown)
library(sqldf)
library("hashmap")
library(stringr)
library(fpp)
library(tm)
library(SnowballC)
library(wordcloud)
library(nlme)
library(tidyr)
library(dplyr)
library(tidytext)
library(ggplot2)
```

```{r setup, include=FALSE}
opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
opts_chunk$set(dev = 'pdf')
```

#outline
intro
data set explantions
  yelp data
  fred
analysis
  yelp users time series
  yelp reviews time series
  yelp review stars time series
  fred data for recession analysis
  yelp stock data
conclusion 

#Introduction

Hypothesis: 
yelpers



#Data Sets

yelp
fred
yahoo finance
bea





#Motivation
If restaurant Yelpers' behavior during and aroud The Great Recession period can be modeled and connected with restaurant expenditures, then there can be conclusions about Yelpers' behavior. If recessionary behavior can be understood, then restaurants can react accordingly.


```{r}
#SETUP
setwd("C:/cygwin64/home/Lester/yelp_challenge_9")

#load data example
#json_file = file("yelp_academic_dataset_checkin.json")
#json_data = jsonlite::stream_in(json_file)
#head(json_data)
#length(json_data$business_id)

load_json = function(filename){
  json_file = file(filename)
  json_data = jsonlite::stream_in(json_file)
  return(json_data)
}

#business = load_json("yelp_academic_dataset_business.json")
#review = load_json("yelp_academic_dataset_review.json")
#checkin = load_json("yelp_academic_dataset_checkin.json")
#tip = load_json("yelp_academic_dataset_tip.json")
#user = load_json("yelp_academic_dataset_user.json")

remove_lists_from_df = function(df){
  i=1
  while(i <= length(df)){
    if(class(df[,i])=="list"){
      df[i] = sapply(df[,i], paste, collapse="|")
    }
    i=i+1
  }
  
  return(df)
}


add_recession_dummy = function(l){
  rec=c()
  for(i in 1:length(l)){
    
    if(l[i]>=as.Date("2007-12-01")&l[i]<=as.Date("2009-07-01")){
      rec=c(rec,1)
    }
    else{
      rec=c(rec,0)
    }
  }
  
  return(rec)
  
}

```

#user data
```{r, include=FALSE, message=FALSE}
user = load_json("yelp_academic_dataset_user.json")
colnames(user)
subset_users = user[4]
subset_users
users_by_date_all = sqldf("select yelping_since, count(yelping_since) from subset_users group by yelping_since order by yelping_since")


#usethis
users_by_date = with(users_by_date_all, users_by_date_all[(users_by_date_all$yelping_since >= "2006" & users_by_date_all$yelping_since < "2017"), ])

users_by_date_xts=xts(users_by_date$`count(yelping_since)`, as.Date(users_by_date$yelping_since,"%Y-%m-%d"))
ts_m = apply.monthly(users_by_date_xts, sum)

ts_m


users_df = data.frame(date=index(ts_m), coredata(ts_m))
users_df

stock_users = with(users_df, users_df[(users_df$date >= "2012-02-01"), ])
```

```{r}
plot(stock_users,type="l")

yelp_stock = get.hist.quote("YELP",end="2017-01-30",quote="AdjClose",compression="m")
# yelp_stock

plot(yelp_stock)

test_stationary = function(t){
  print(kpss.test(t))
  print(adf.test(t))
}

test_stationary(ts(stock_users$coredata.ts_m.,start=c(2012,3),freq=12))
test_stationary(yelp_stock)

log_user_growth = as.data.frame(diff(log(stock_users$coredata.ts_m.)))

log_yelp_growth = as.data.frame(diff(log(yelp_stock)))

# log_user_growth
# log_yelp_growth




ts_users = ts(log_user_growth,start=c(2012,4),freq=12)
test_stationary(ts_users)

ts_yelp = ts(log_yelp_growth,start=c(2012,4),freq=12)
test_stationary(ts_yelp)

plot(ts_users,col="blue")
lines(ts_yelp,col="red")

combined = cbind(ts_yelp,ts_users)
select=VARselect(combined,lag.max=12,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm=VAR(combined,p=select$select[1])
plot(vm$y)
summary(vm)

grangertest(ts_users~ts_yelp)



sp500 = get.hist.quote("^GSPC",quote="AdjClose",compression="m",start=as.Date("2012-03-01","%Y-%m-%d"),end=as.Date("2017-01-30","%Y-%m-%d"))

# sp500
test_stationary(sp500)

log_sp500_growth = as.data.frame(diff(log(sp500)))
ts_sp500 = ts(log_sp500_growth,start=c(2012,4),freq=12)
# ts_sp500
test_stationary(ts_sp500)

# ts_yelp
# ts_sp500


plot(ts_users,col="blue")
lines(ts_yelp,col="red")

combined = cbind(ts_yelp,ts_sp500)

select=VARselect(combined,lag.max=12,type=c("const","trend","both","none"),season=NULL,exogen=NULL)

vm=VAR(combined,p=select$select[1])

plot(vm$y)

summary(vm)



```

Growth rate of users do not have an effect on the stock value of Yelp - no correlation to stock market. good! This elinates the possibility that effects are due to yelp as a company doing good or bad.

should i examine review scores + users + etc and its relation to s and p500 during recession? or just move on to gdp and recession

#review scores and gdp

```{r, include=FALSE, message=FALSE}

review = load_json("yelp_academic_dataset_review.json")

#join with restaurant reviews
business = load_json("yelp_academic_dataset_business.json")
business = remove_lists_from_df(business)

#sanity check the states for subsetting into states
sanity_state = sqldf("select state from business group by state")
sanity_state

#us only
restaurants = sqldf("select business_id from business where (categories like '%Restaurants%' and (state = 'AZ' or state = 'IL' or state = 'NC' or state = 'NV' or state = 'NY' or state = 'OH' or state = 'PA' or state = 'SC' or state = 'VT' or state = 'WI'))")

#us only
restaurant_reviews_all = sqldf("select * from review join restaurants on review.business_id = restaurants.business_id")
colnames(restaurant_reviews_all) = c("review_id","user_id","b_id","stars","date","text","useful","funny","cool","type","business_id")

#use this for reviews, start from 2006 for larger sample sizes (<2006 extremely low sample sizes & heterskedasticity)
restaurant_reviews = with(restaurant_reviews_all, restaurant_reviews_all[(restaurant_reviews_all$date >= "2006" & restaurant_reviews_all$date < "2017"), ])


review_date_star = as.data.frame(restaurant_reviews$date)
review_date_star$stars = restaurant_reviews$stars
head(review_date_star)
colnames(review_date_star)=c("date","stars")


review_date_star$date=as.Date(review_date_star$date,"%Y-%m-%d")

#freq of new reviews
review_counts_by_date = sqldf("select date, count(date) from review_date_star group by date order by date")

head(review_counts_by_date)

```

```{r}
plot(review_counts_by_date$date,review_counts_by_date$`count(date)`,type="l")

#convert to monthly
reviews_by_date=xts(review_counts_by_date$`count(date)`, as.Date(review_counts_by_date$date,"%Y-%m-%d"))
df_rev_m = apply.monthly(reviews_by_date, sum)
df_rev_count = data.frame(date=index(df_rev_m), coredata(df_rev_m))
# df_rev_count

plot(df_rev_count$date,df_rev_count$coredata.df_rev_m.,type="l")


#review growth rates/new user growth rates
log_rev_count=diff(log(df_rev_count$coredata.df_rev_m.))
#log_rev_count[1]=NA
log_rev_count = na.omit(log_rev_count)
log_rev_count = ts(log_rev_count,start=c(2006,2),freq=12)

# ts_m
log_user_count=diff(log(ts_m[,1]))
log_user_count = na.omit(log_user_count)
log_user_count = ts(log_user_count,start=c(2006,2),freq=12)

plot(log_rev_count,type="l", main="growth rate of user reviews and accounts")
lines(log_user_count[,1],col="red")

#do a var model between growth rate of users revs and accounts

#create var of growth rates
rates_combined = cbind(log_rev_count,log_user_count)
select=VARselect(rates_combined,lag.max=12,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_rates=VAR(rates_combined,select$select[1])
plot(vm_rates$y)
summary(vm_rates)

#do granger causality test
grangertest(log_rev_count~log_user_count,order=select$select[1])
grangertest(log_user_count~log_rev_count,order=select$select[1])

user_rev_lm=lm(log_rev_count~log_user_count)
summary(user_rev_lm)

#users and review#s granger cause each other, just use one

#stars by month
stars_by_date=xts(review_date_star$stars, as.Date(review_date_star$date,"%Y-%m-%d"))
df_stars_m = apply.monthly(stars_by_date, sum)
df_stars = data.frame(date=index(df_stars_m), coredata(df_stars_m))


df_stars$avg = df_stars$coredata.df_stars_m./df_rev_count$coredata.df_rev_m.
# head(df_stars)

plot(df_stars$date,df_stars$avg,type = "l")
#evidence of recession in stars
stars_recession_dummy = add_recession_dummy(df_stars$date)
stars_avg=df_stars$avg
stars_lm=lm(stars_avg~stars_recession_dummy)
summary(stars_lm)



test_stationary(df_stars$avg)
#convert to growth rates
df_stars_diff_log = as.data.frame(diff(log(df_stars$avg)))
df_stars_diff_log$date=df_stars$date[2:length(df_stars$date)]

df_stars_diff_log
colnames(df_stars_diff_log)=c("avg","date")

test_stationary(df_stars_diff_log$avg)


stars_recession_diff_log_dummy = add_recession_dummy(df_stars_diff_log$date)

# stars_reg = lm(df_stars_diff_log$avg ~ stars_recession_diff_log_dummy)
# summary(stars_reg)

plot(df_stars_diff_log$date,df_stars_diff_log$avg,type="l")

stars_diff_log_avg = df_stars_diff_log$avg
stars_diff_log_lm = lm(stars_diff_log_avg~stars_recession_diff_log_dummy)
summary(stars_diff_log_lm)


#however, intuitively we should be looking at level, not growth rates
#so lets detrend the data and season

ts_stars = ts(df_stars$avg,start=c(2006,2),freq=12)

stars_tslm = tslm(ts_stars~trend+season)
summary(stars_tslm)

plot(ts_stars)
lines(stars_tslm$fitted.values,col="red")

detrend_stars = resid(stars_tslm)
plot(detrend_stars)

detrend_stars_lm=lm(detrend_stars~stars_recession_dummy)
summary(detrend_stars_lm)




```

the growth rates plot shows that even during recession we don't see a dip in either. if there was constant account creation but a dip in reviews, that would mean the recession has an effect on num of reviews






#descriptive stats of reviews and stars

```{r}
sd(df_rev_count$coredata.df_rev_m.)
mean(df_rev_count$coredata.df_rev_m.)


sd(log_rev_count)
mean(log_rev_count)


sd(df_stars$avg)
mean(df_stars$avg)

```

#fred data

```{r}




#real gdp
getSymbols('GDPC96', src = 'FRED')
gdp = GDPC96

plot(gdp)

gdp_growth = na.omit(diff(log(gdp)))
plot(gdp_growth)
gdp_growth_subset = with(gdp_growth, gdp_growth[index(gdp_growth) >= "2006-04-01" & index(gdp_growth) < "2016-12-30", ])
gdp_growth_subset = ts(gdp_growth_subset,start=c(2006,2),frequency = 4)
plot(gdp_growth_subset)

#create quarterly review growth rate
reviews_by_quarter=xts(review_counts_by_date$`count(date)`, as.Date(review_counts_by_date$date,"%Y-%m-%d"))
df_rev_m_quarter = apply.quarterly(reviews_by_quarter, sum)
df_rev_quarter = data.frame(date=index(df_rev_m_quarter), coredata(df_rev_m_quarter))
# df_rev_quarter
plot(df_rev_quarter$date,df_rev_quarter$coredata.df_rev_m_quarter.,type="l")

log_rev_quarter=diff(log(df_rev_quarter$coredata.df_rev_m_quarter.))
log_rev_quarter = na.omit(log_rev_quarter)
log_rev_quarter = ts(log_rev_quarter,start=c(2006,2),freq=4)
plot(log_rev_quarter,type="l")

# length(log_rev_quarter)
# length(gdp_growth_subset)

#var of gdp and user reviews growth rates
gdp_combined = cbind(log_rev_quarter,gdp_growth_subset)
select=VARselect(gdp_combined,lag.max=12,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_gdp=VAR(gdp_combined,p=12)
plot(vm_gdp$y)
summary(vm_gdp)

#do a var on quarterly gdp growth and quarterly avg review score

recession_dummy_reviews_q = add_recession_dummy(df_rev_quarter$date)

reg_reviews = lm(df_rev_quarter$coredata.df_rev_m_quarter.~recession_dummy_reviews_q)
summary(reg_reviews)
#first reg is misleading



#look at growth rates

rec_dummy_rev_growth_q = recession_dummy_reviews_q[2:length(recession_dummy_reviews_q)]

reg_log_reviews = lm(log_rev_quarter~rec_dummy_rev_growth_q)
summary(reg_log_reviews)

#use lm
log_reviews_lm = lm(log_rev_quarter~rec_dummy_rev_growth_q)
summary(log_reviews_lm)

```




#examine social dynamics during recessions through dollar signs as well as review texts
do people eat at cheaper places? do people care more about overpriced food?
```{r, include=FALSE, message=FALSE}
#convert $ dollar signs to time series
restaurant_visits = sqldf("select restaurant_reviews.date, business.attributes, restaurant_reviews.stars from restaurant_reviews join business on restaurant_reviews.business_id = business.business_id")

#dollar sign extractor
get_dollar_signs=function(s){
  temp=str_extract(s,"RestaurantsPriceRange2(.*?)[0-9]")
  return(substr(temp,nchar(temp),nchar(temp)))
}


restaurant_dollars=data.frame(restaurant_visits$date)
restaurant_dollars$dollars = get_dollar_signs(restaurant_visits$attributes)

restaurant_dollars=na.omit(restaurant_dollars)
class(restaurant_dollars$restaurant_visits.date)
restaurant_dollars$restaurant_visits.date=as.character(restaurant_dollars$restaurant_visits.date)
colnames(restaurant_dollars) = c("date","dollars")

dollars_1 = sqldf("select * from restaurant_dollars where dollars=1")
dollars_2 = sqldf("select * from restaurant_dollars where dollars=2")
dollars_3 = sqldf("select * from restaurant_dollars where dollars=3")
dollars_4 = sqldf("select * from restaurant_dollars where dollars=4")

dollars_gbd_1 = sqldf("select date, count(date) from dollars_1 group by date order by date")
dollars_gbd_2 = sqldf("select date, count(date) from dollars_2 group by date order by date")
dollars_gbd_3 = sqldf("select date, count(date) from dollars_3 group by date order by date")
dollars_gbd_4 = sqldf("select date, count(date) from dollars_4 group by date order by date")

```

```{r}

dollars_1_xts=xts(dollars_gbd_1$`count(date)`, as.Date(dollars_gbd_1$date,"%Y-%m-%d"))
df_d_1 = apply.monthly(dollars_1_xts, sum)
df_dollars_1 = data.frame(date=index(df_d_1), coredata(df_d_1))
# df_dollars_1

dollars_2_xts=xts(dollars_gbd_2$`count(date)`, as.Date(dollars_gbd_2$date,"%Y-%m-%d"))
df_d_2 = apply.monthly(dollars_2_xts, sum)
df_dollars_2 = data.frame(date=index(df_d_2), coredata(df_d_2))
# df_dollars_2

dollars_3_xts=xts(dollars_gbd_3$`count(date)`, as.Date(dollars_gbd_3$date,"%Y-%m-%d"))
df_d_3 = apply.monthly(dollars_3_xts, sum)
df_dollars_3 = data.frame(date=index(df_d_3), coredata(df_d_3))
# df_dollars_3

dollars_4_xts=xts(dollars_gbd_4$`count(date)`, as.Date(dollars_gbd_4$date,"%Y-%m-%d"))
df_d_4 = apply.monthly(dollars_4_xts, sum)
df_dollars_4 = data.frame(date=index(df_d_4), coredata(df_d_4))
# df_dollars_4


recession_dummy_dollars_m = add_recession_dummy(df_dollars_1$date)


plot(df_dollars_1,type="l")
plot(df_dollars_2,type="l")
plot(df_dollars_3,type="l")
plot(df_dollars_4,type="l")

```

```{r}
df_dollars_1_dlog = as.data.frame(diff(log(df_dollars_1$coredata.df_d_1.)))
df_dollars_2_dlog = as.data.frame(diff(log(df_dollars_2$coredata.df_d_2.)))
df_dollars_3_dlog = as.data.frame(diff(log(df_dollars_3$coredata.df_d_3.)))
df_dollars_4_dlog = as.data.frame(diff(log(df_dollars_4$coredata.df_d_4.)))


#ts of dlogs
ts_dollar_1=ts(df_dollars_1$coredata.df_d_1.,start=c(2006,2),freq=12)
ts_dollar_2=ts(df_dollars_2$coredata.df_d_2.,start=c(2006,2),freq=12)
ts_dollar_3=ts(df_dollars_3$coredata.df_d_3.,start=c(2006,2),freq=12)
ts_dollar_4=ts(df_dollars_4$coredata.df_d_4.,start=c(2006,2),freq=12)


dollars_1_dlog=diff(log(ts_dollar_1))
dollars_2_dlog=diff(log(ts_dollar_2))
dollars_3_dlog=diff(log(ts_dollar_3))
dollars_4_dlog=diff(log(ts_dollar_4))

plot(dollars_1_dlog)
plot(dollars_2_dlog)
plot(dollars_3_dlog)
plot(dollars_4_dlog)

lm_dollars_recession_dummy = recession_dummy_dollars_m[2:length(recession_dummy_dollars_m)]

```

#seasonally adjust the data

```{r}

#seasonally adjust data

#levels
tslm_d1 = tslm(ts_dollar_1~trend+season)
#summary(tslm_d1)
tslm_d1_resid = resid(tslm_d1)
plot(tslm_d1_resid)
lm_d1_adj = lm(tslm_d1_resid~recession_dummy_dollars_m)
summary(lm_d1_adj)

tslm_d2 = tslm(ts_dollar_2~trend+season)
#summary(tslm_d1)
tslm_d2_resid = resid(tslm_d2)
plot(tslm_d2_resid)
lm_d2_adj = lm(tslm_d2_resid~recession_dummy_dollars_m)
summary(lm_d2_adj)

tslm_d3 = tslm(ts_dollar_3~trend+season)
#summary(tslm_d1)
tslm_d3_resid = resid(tslm_d3)
plot(tslm_d3_resid)
lm_d3_adj = lm(tslm_d3_resid~recession_dummy_dollars_m)
summary(lm_d3_adj)

tslm_d4 = tslm(ts_dollar_4~trend+season)
#summary(tslm_d1)
tslm_d4_resid = resid(tslm_d4)
plot(tslm_d4_resid)
lm_d4_adj = lm(tslm_d4_resid~recession_dummy_dollars_m)
summary(lm_d4_adj)

```

```{r}


#growth
tslm_d1_growth = tslm(dollars_1_dlog~trend+season)
#summary(tslm_d1)
tslm_d1_growth_resid = resid(tslm_d1_growth)
plot(tslm_d1_growth_resid)
lm_d1_growth_adj = lm(tslm_d1_growth_resid~lm_dollars_recession_dummy)
summary(lm_d1_growth_adj)

tslm_d2_growth = tslm(dollars_2_dlog~trend+season)
#summary(tslm_d1)
tslm_d2_growth_resid = resid(tslm_d2_growth)
plot(tslm_d2_growth_resid)
lm_d2_growth_adj = lm(tslm_d2_growth_resid~lm_dollars_recession_dummy)
summary(lm_d2_growth_adj)

tslm_d3_growth = tslm(dollars_3_dlog~trend+season)
#summary(tslm_d1)
tslm_d3_growth_resid = resid(tslm_d3_growth)
plot(tslm_d3_growth_resid)
lm_d3_growth_adj = lm(tslm_d3_growth_resid~lm_dollars_recession_dummy)
summary(lm_d3_growth_adj)

tslm_d4_growth = tslm(dollars_4_dlog~trend+season)
#summary(tslm_d1)
tslm_d4_growth_resid = resid(tslm_d4_growth)
plot(tslm_d4_growth_resid)
lm_d4_growth_adj = lm(tslm_d4_growth_resid~lm_dollars_recession_dummy)
summary(lm_d4_growth_adj)

```

```{r}

#percentages
ts_dollar_total = ts_dollar_1+ts_dollar_2+ts_dollar_3+ts_dollar_4
ts_d1_per = ts_dollar_1/ts_dollar_total
ts_d2_per = ts_dollar_2/ts_dollar_total
ts_d3_per = ts_dollar_3/ts_dollar_total
ts_d4_per = ts_dollar_4/ts_dollar_total

plot(ts_d1_per)
plot(ts_d2_per)
plot(ts_d3_per)
plot(ts_d4_per)

tslm_d1_per = tslm(ts_d1_per~trend+season)
tslm_d1_per_resid = resid(tslm_d1_per)
plot(tslm_d1_per_resid)
lm_d1_per_adj = lm(tslm_d1_per_resid~recession_dummy_dollars_m)
summary(lm_d1_per_adj)

tslm_d2_per = tslm(ts_d2_per~trend+season)
tslm_d2_per_resid = resid(tslm_d2_per)
plot(tslm_d2_per_resid)
lm_d2_per_adj = lm(tslm_d2_per_resid~recession_dummy_dollars_m)
summary(lm_d2_per_adj)

tslm_d3_per = tslm(ts_d3_per~trend+season)
tslm_d3_per_resid = resid(tslm_d3_per)
plot(tslm_d3_per_resid)
lm_d3_per_adj = lm(tslm_d3_per_resid~recession_dummy_dollars_m)
summary(lm_d3_per_adj)

tslm_d4_per = tslm(ts_d4_per~trend+season)
tslm_d4_per_resid = resid(tslm_d4_per)
plot(tslm_d4_per_resid)
lm_d4_per_adj = lm(tslm_d4_per_resid~recession_dummy_dollars_m)
summary(lm_d4_per_adj)


```

#introduce num of reviews into model
control for number of new reviews, to make sure that the recession dummy doesn't accidentally capture an effect from new review numbers, in other words if the number of reviews dropped it would make sense that the number of reviews for a certain amount of dollar signs drops too

```{r}
#levels
lm_d1_adj_rev = lm(tslm_d1_resid~recession_dummy_dollars_m+df_rev_count$coredata.df_rev_m.)
summary(lm_d1_adj_rev)

lm_d2_adj_rev = lm(tslm_d2_resid~recession_dummy_dollars_m+df_rev_count$coredata.df_rev_m.)
summary(lm_d2_adj_rev)

lm_d3_adj_rev = lm(tslm_d3_resid~recession_dummy_dollars_m+df_rev_count$coredata.df_rev_m.)
summary(lm_d3_adj_rev)

lm_d4_adj_rev = lm(tslm_d4_resid~recession_dummy_dollars_m+df_rev_count$coredata.df_rev_m.)
summary(lm_d4_adj_rev)


```


#re examine stars, but subset by dollar signs

```{r, include=FALSE, message=FALSE}

stars_by_restaurant_dollars = data.frame(restaurant_visits$date)
stars_by_restaurant_dollars$dollars = get_dollar_signs(restaurant_visits$attributes)
stars_by_restaurant_dollars$stars = restaurant_visits$stars

stars_by_restaurant_dollars=na.omit(stars_by_restaurant_dollars)
stars_by_restaurant_dollars$restaurant_visits.date=as.character(stars_by_restaurant_dollars$restaurant_visits.date)
colnames(stars_by_restaurant_dollars) = c("date","dollars","stars")

dollars_1_star = sqldf("select * from stars_by_restaurant_dollars where dollars=1")
dollars_2_star = sqldf("select * from stars_by_restaurant_dollars where dollars=2")
dollars_3_star = sqldf("select * from stars_by_restaurant_dollars where dollars=3")
dollars_4_star = sqldf("select * from stars_by_restaurant_dollars where dollars=4")

dollars_obd_1_star = sqldf("select date, stars from dollars_1_star order by date")
dollars_obd_2_star = sqldf("select date, stars from dollars_2_star order by date")
dollars_obd_3_star = sqldf("select date, stars from dollars_3_star order by date")
dollars_obd_4_star = sqldf("select date, stars from dollars_4_star order by date")
  
```

```{r}

dollars_1_star_xts=xts(dollars_obd_1_star$stars, as.Date(dollars_obd_1_star$date,"%Y-%m-%d"))
df_d_1_star = apply.monthly(dollars_1_star_xts, sum)
df_dollars_1_star = data.frame(date=index(df_d_1_star), coredata(df_d_1_star))
df_dollars_1_star$avg = df_dollars_1_star$coredata.df_d_1_star./df_dollars_1$coredata.df_d_1.
plot(df_dollars_1_star$date,df_dollars_1_star$avg)
d1_star_lm = lm(df_dollars_1_star$avg~recession_dummy_dollars_m)
summary(d1_star_lm)

dollars_2_star_xts=xts(dollars_obd_2_star$stars, as.Date(dollars_obd_2_star$date,"%Y-%m-%d"))
df_d_2_star = apply.monthly(dollars_2_star_xts, sum)
df_dollars_2_star = data.frame(date=index(df_d_2_star), coredata(df_d_2_star))
df_dollars_2_star$avg = df_dollars_2_star$coredata.df_d_2_star./df_dollars_2$coredata.df_d_2.
plot(df_dollars_2_star$date,df_dollars_2_star$avg)
d2_star_lm = lm(df_dollars_2_star$avg~recession_dummy_dollars_m)
summary(d2_star_lm)

dollars_3_star_xts=xts(dollars_obd_3_star$stars, as.Date(dollars_obd_3_star$date,"%Y-%m-%d"))
df_d_3_star = apply.monthly(dollars_3_star_xts, sum)
df_dollars_3_star = data.frame(date=index(df_d_3_star), coredata(df_d_3_star))
df_dollars_3_star$avg = df_dollars_3_star$coredata.df_d_3_star./df_dollars_3$coredata.df_d_3.
plot(df_dollars_3_star$date,df_dollars_3_star$avg)
d3_star_lm = lm(df_dollars_3_star$avg~recession_dummy_dollars_m)
summary(d3_star_lm)

dollars_4_star_xts=xts(dollars_obd_4_star$stars, as.Date(dollars_obd_4_star$date,"%Y-%m-%d"))
df_d_4_star = apply.monthly(dollars_4_star_xts, sum)
df_dollars_4_star = data.frame(date=index(df_d_4_star), coredata(df_d_4_star))
df_dollars_4_star$avg = df_dollars_4_star$coredata.df_d_4_star./df_dollars_4$coredata.df_d_4.
plot(df_dollars_4_star$date,df_dollars_4_star$avg)
d4_star_lm = lm(df_dollars_4_star$avg~recession_dummy_dollars_m)
summary(d4_star_lm)

#looks like the stars dropping during a recession was only in 1 and 2 dollar signs restaurants

```


#comment out for now, cuz takes too long to process


analyze word usages
```{r}

buildCorpus = function(data, stem){
  corpus = Corpus(VectorSource(data))
  corpus = tm_map(corpus, content_transformer(tolower))
  corpus = tm_map(corpus, PlainTextDocument)
  corpus = tm_map(corpus, removePunctuation)
  corpus = tm_map(corpus, removeWords, stopWords)
  if(stem==1) corpus = tm_map(corpus, stemDocument)
  return(corpus)
}

buildWordCloud = function(corpus, pal,val, name){
  wordcloud(corpus, max.words = 75, random.order = FALSE, colors=brewer.pal(val,pal), main = name)
}
stopWords = removePunctuation(stopwords('SMART'))


restaurant_reviews_rec = with(restaurant_reviews, restaurant_reviews[(restaurant_reviews$date >= "2007-12" & restaurant_reviews$date <= "2009-06"), ])

restaurant_reviews_norec = with(restaurant_reviews, restaurant_reviews[(restaurant_reviews$date > "2009-06" & restaurant_reviews$date <= "2011-12"), ])

#create corupses
corpus_reviews_rec = buildCorpus(restaurant_reviews_rec$text,0)
dtm_rec = DocumentTermMatrix(corpus_reviews_rec)
tidy_rec = tidy(dtm_rec)
ap_sentiments <- tidy_rec %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

ap_sentiments

ap_sentiments %>%
  count(document, sentiment, wt = count) %>%
  ungroup() %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(sentiment)


ap_sentiments %>%
  count(sentiment, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 2000) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")

#negative: 158714
#positive: 346991
#percent negative: 31.4%
#cheap: 3rd highest negative word
#expensive: 7th



corpus_reviews_norec = buildCorpus(restaurant_reviews_norec$text,0)
dtm_norec = DocumentTermMatrix(corpus_reviews_norec)
tidy_norec = tidy(dtm_norec)
ap_sentiments <- tidy_norec %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

ap_sentiments

ap_sentiments %>%
  count(document, sentiment, wt = count) %>%
  ungroup() %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(sentiment)

ap_sentiments %>%
  count(sentiment, term, wt = count) %>%
  ungroup() %>%
  filter(n >= 7500) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")

#negative: 733329
#positive: 1731938
#percent negative: 29.75% 
#cheap: 5th
#expensive: 8th

buildWordCloud(corpus_reviews_rec, "Spectral",8, "test")
buildWordCloud(corpus_reviews_norec, "Spectral",8, "test2")


```





#connect with restaurants

```{r, include=FALSE, message=FALSE}

restaurant_expenditures=read.csv("restaurant_expenditures.csv")
# restaurant_expenditures

```

```{r}

#add x axis with dates
rest_exp_dates = seq(as.Date("2005/12/01"), by = "quarter",length.out = 44)

plot(rest_exp_dates,restaurant_expenditures$real_exp,type="l")

test_stationary(restaurant_expenditures$real_exp)

rest_real_exp_diff_log = diff(log(restaurant_expenditures$real_exp))

test_stationary(rest_real_exp_diff_log)


rest_exp_dates_diff = rest_exp_dates[2:length(rest_exp_dates)]
plot(rest_exp_dates_diff,rest_real_exp_diff_log,type="l")




#create var + granger causality for real exp and gdp
gdp_exp_combined = cbind(rest_real_exp_diff_log,gdp_growth_subset)
select=VARselect(gdp_exp_combined,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_gdp_exp=VAR(gdp_exp_combined,select$select[1])
plot(vm_gdp_exp$y)
summary(vm_gdp_exp)

grangertest(rest_real_exp_diff_log~gdp_growth_subset[1:length(gdp_growth_subset)],order=select$select[1])
grangertest(gdp_growth_subset[1:length(gdp_growth_subset)]~rest_real_exp_diff_log,order=select$select[1])

rec_exp_diff_log_dummy = add_recession_dummy(rest_exp_dates_diff)
lm_rest_real_exp_diff_log = lm(rest_real_exp_diff_log~rec_exp_diff_log_dummy)
summary(lm_rest_real_exp_diff_log)

```

yes gdp granger causes restaurant expenditures and recession dummy



#connect num of reviews with restaurants





```{r}

rev_exp_combined = cbind(rest_real_exp_diff_log,log_rev_quarter)
select=VARselect(rev_exp_combined,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_rev_exp=VAR(rev_exp_combined,select$select[1])
plot(vm_rev_exp$y)
summary(vm_rev_exp)

grangertest(rest_real_exp_diff_log~log_rev_quarter, order=select$select[1])
grangertest(log_rev_quarter~rest_real_exp_diff_log, order=select$select[1])


#looks like log_rev_quarter is granger caused by rest_real_exp_diff_log
log_rev_exp_lm = lm(log_rev_quarter~rec_dummy_rev_growth_q+rest_real_exp_diff_log)
summary(log_rev_exp_lm)
plot(log_rev_exp_lm$fitted,type="l")
plot(log_rev_quarter)


#gdp IV

#two stage regression
stage1 = lm(rest_real_exp_diff_log~gdp_growth_subset)
summary(stage1)
stage1_fitted = fitted(stage1)

#var for possible iv
iv_combined = cbind(stage1_fitted,log_rev_quarter)
select=VARselect(iv_combined,lag.max=4,type=c("const","trend","both","none"),season=NULL,exogen=NULL)
vm_iv=VAR(iv_combined,select$select[1])
plot(vm_iv$y)
summary(vm_iv)

grangertest(stage1_fitted~log_rev_quarter,order=select$select[1])
grangertest(log_rev_quarter~stage1_fitted,order=select$select[1])
```












old stuff



<!-- #Exploring the Data -->


<!-- ```{r, echo=FALSE} -->
<!-- business = remove_lists_from_df(business) -->

<!-- business_us = sqldf("select * from business where (state = 'AZ' or state = 'IL'  -->
<!--                     or state = 'NC' or state = 'NV' or state = 'NY' or state = 'OH'  -->
<!--                     or state = 'PA' or state = 'SC' or state = 'VT' or state = 'WI')") -->

<!-- restaurants = sqldf("select * from business where categories like '%Restaurants%'") -->
<!-- restaurants_open = sqldf("select * from restaurants where is_open=1") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #number of states -->
<!-- length(sqldf("select state from restaurants_open group by state")[,1]) -->
<!-- #number of cities -->
<!-- length(sqldf("select city, state from restaurants_open group by city")[,1]) -->
<!-- ``` -->

<!-- After examining the states and cities, there are some omissions, such as Los Angeles and New York City, that could have been extremely interesting if included. Nevertheless, let's see how the distribution of cities look. -->

<!-- ```{r} -->
<!-- city_counts = sqldf("select city, state, count(business_id) as count from restaurants_open group by city") -->

<!-- #ones, 100s, 200s, ..., 1000+ -->
<!-- buckets = c(0,0,0,0,0,0,0,0,0,0,0) -->

<!-- i=1 -->
<!-- while(i <= length(city_counts[,1])){ -->
<!--   temp_count = city_counts$count[i] -->
<!--   if(temp_count<100){ -->
<!--     buckets[1]=buckets[1]+1 -->
<!--   } -->
<!--   else if(temp_count>=100 & temp_count < 200){ -->
<!--     buckets[2]=buckets[2]+1 -->
<!--   } -->
<!--   else if(temp_count>=200 & temp_count < 300){ -->
<!--     buckets[3]=buckets[3]+1 -->
<!--   } -->
<!--   else if(temp_count>=300 & temp_count < 400){ -->
<!--     buckets[4]=buckets[4]+1 -->
<!--   } -->
<!--   else if(temp_count>=400 & temp_count < 500){ -->
<!--     buckets[5]=buckets[5]+1 -->
<!--   } -->
<!--   else if(temp_count>=500 & temp_count < 600){ -->
<!--     buckets[6]=buckets[6]+1 -->
<!--   } -->
<!--   else if(temp_count>=600 & temp_count < 700){ -->
<!--     buckets[7]=buckets[7]+1 -->
<!--   } -->
<!--   else if(temp_count>=700 & temp_count < 800){ -->
<!--     buckets[8]=buckets[8]+1 -->
<!--   } -->
<!--   else if(temp_count>=800 & temp_count < 900){ -->
<!--     buckets[9]=buckets[9]+1 -->
<!--   } -->
<!--   else if(temp_count>=900 & temp_count < 1000){ -->
<!--     buckets[10]=buckets[10]+1 -->
<!--   } -->
<!--   else if(temp_count>=1000){ -->
<!--     buckets[11]=buckets[11]+1 -->
<!--   } -->
<!--   else{ -->
<!--     print(paste(c("ERROR at index: ",i))) -->
<!--   } -->
<!--   i=i+1 -->
<!-- } -->

<!-- plot(buckets) -->

<!-- ``` -->
<!-- ```{r} -->

<!-- mean(city_counts$count) -->
<!-- sd(city_counts$count) -->
<!-- quantile(city_counts$count) -->

<!-- ``` -->

<!-- Looks like most of the cities fall into the first bucket of less than 100 businesses. However, we want to narrow our search to just large cities, so let's remove that bucket. -->

<!-- ```{r} -->
<!-- city_counts2 = sqldf("select * from city_counts where count >100") -->
<!-- buckets2 = buckets[2:length(buckets)] -->
<!-- plot(buckets2) -->
<!-- ``` -->

<!-- ```{r} -->

<!-- mean(city_counts2$count) -->
<!-- sd(city_counts2$count) -->
<!-- quantile(city_counts2$count) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- city_counts3 = sqldf("select * from city_counts where count >100 and count <1000") -->
<!-- mean(city_counts3$count) -->
<!-- sd(city_counts3$count) -->
<!-- quantile(city_counts3$count) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #subset by >245 obs -->
<!-- large_cities = sqldf("select city, state, count(business_id) as count from restaurants_open group by city having count > 245") -->
<!-- large_cities -->
<!-- ``` -->

<!-- Let's take a look at demographics for our potential cities: -->

<!-- ```{r} -->

<!-- demographics = read.csv("demographics.csv",stringsAsFactors = FALSE) -->
<!-- sqldf("select city, pop_by_race_2015 from demographics order by pop_by_race_2015") -->
<!-- ``` -->

<!-- We can see that Champaign definately doesn't meet our criteria for city size, so I'll drop it. Althought Tempe isn't as large as some of the other cities, I will make a judgement call and still group it as a upper mid to large sized city. -->

<!-- ```{r} -->

<!-- #drop champaign -->
<!-- demographics = demographics[-1,] -->
<!-- demographics -->
<!-- ``` -->

<!-- ```{r} -->

<!-- #find all occurances of restaurant types in each location -->

<!-- cities_usethis = sqldf("select city from demographics") -->
<!-- restaurants_usethis = sqldf("select * from restaurants_open where (city='Chandler' or -->
<!--                             city='Charlotte' or city='Cleveland' or city='Gilbert' -->
<!--                             or city='Glendale' or city='Henderson' or city='Las Vegas' -->
<!--                             or city='Madison' or city='Mesa' or city='Phoenix' -->
<!--                             or city='Pittsburgh' or city='Scottsdale' or city='Tempe')") -->

<!-- get_city=function(df,i){ -->
<!--   return(df$city[i]) -->
<!-- } -->


<!-- create_hashmap = function(){ -->
<!--   key="type" -->
<!--   value=0 -->
<!--   temp = hashmap(key,value) -->
<!--   temp$erase(key) -->
<!--   return(temp) -->
<!-- } -->

<!-- hmap_chandler = create_hashmap() -->
<!-- hmap_charlotte = create_hashmap() -->
<!-- hmap_cleveland = create_hashmap() -->
<!-- hmap_gilbert = create_hashmap() -->
<!-- hmap_glendale = create_hashmap() -->
<!-- hmap_henderson = create_hashmap() -->
<!-- hmap_lasvegas = create_hashmap() -->
<!-- hmap_madison = create_hashmap() -->
<!-- hmap_mesa = create_hashmap() -->
<!-- hmap_phoenix = create_hashmap() -->
<!-- hmap_pittsburgh = create_hashmap() -->
<!-- hmap_scottsdale = create_hashmap() -->
<!-- hmap_tempe = create_hashmap() -->

<!-- #takes string, delim by pipe |, counts occurances, updates hmap -->
<!-- update_hashmap = function(hmap,s){ -->
<!--   vec=unlist(strsplit(s,split="\\|")) -->
<!--   len = length(vec) -->
<!--   i = 1 -->
<!--   while(i<=len){ -->

<!--     if(hmap$has_key(vec[i])){ -->
<!--       temp = hmap$find(vec[i]) -->
<!--       temp = temp+1 -->
<!--       hmap$insert(vec[i],temp) -->
<!--     } -->
<!--     else{  -->
<!--       hmap$insert(vec[i],1) -->
<!--     } -->
<!--     i=i+1 -->
<!--   } -->
<!--   return(hmap) -->
<!-- } -->

<!-- i = 1 -->
<!-- while(i <= length(restaurants_usethis$business_id)){ -->
<!--   city=get_city(restaurants_usethis,i) -->
<!--   cat = restaurants_usethis$categories[i] -->
<!--   if(city=="Chandler"){ -->
<!--     hmap_chandler=update_hashmap(hmap_chandler,cat) -->
<!--   } -->
<!--   else if(city=="Charlotte"){ -->
<!--     hmap_charlotte=update_hashmap(hmap_charlotte,cat) -->
<!--   } -->
<!--   else if(city=="Cleveland"){ -->
<!--     hmap_cleveland=update_hashmap(hmap_cleveland,cat) -->
<!--   } -->
<!--   else if(city=="Gilbert"){ -->
<!--     hmap_gilbert=update_hashmap(hmap_gilbert,cat) -->
<!--   } -->
<!--   else if(city=="Glendale"){ -->
<!--     hmap_glendale=update_hashmap(hmap_glendale,cat) -->
<!--   } -->
<!--   else if(city=="Henderson"){ -->
<!--     hmap_henderson=update_hashmap(hmap_henderson,cat) -->
<!--   } -->
<!--   else if(city=="Las Vegas"){ -->
<!--     hmap_lasvegas=update_hashmap(hmap_lasvegas,cat) -->
<!--   } -->
<!--   else if(city=="Madison"){ -->
<!--     hmap_madison=update_hashmap(hmap_madison,cat) -->
<!--   } -->
<!--   else if(city=="Mesa"){ -->
<!--     hmap_mesa=update_hashmap(hmap_mesa,cat) -->
<!--   } -->
<!--   else if(city=="Phoenix"){ -->
<!--     hmap_phoenix=update_hashmap(hmap_phoenix,cat) -->
<!--   } -->
<!--   else if(city=="Pittsburgh"){ -->
<!--     hmap_pittsburgh=update_hashmap(hmap_pittsburgh,cat) -->
<!--   } -->
<!--   else if(city=="Scottsdale"){ -->
<!--     hmap_scottsdale=update_hashmap(hmap_scottsdale,cat) -->
<!--   } -->
<!--   else if(city=="Tempe"){ -->
<!--     hmap_tempe=update_hashmap(hmap_tempe,cat) -->
<!--   } -->
<!--   else{ -->
<!--     print(paste(c("ERROR at index: ",i))) -->
<!--   } -->


<!--   i=i+1 -->
<!-- } -->



<!-- ``` -->

<!-- ```{r} -->

<!-- hmap_vec = c(hmap_chandler,hmap_charlotte,hmap_cleveland,hmap_gilbert, -->
<!--              hmap_glendale,hmap_henderson,hmap_lasvegas,hmap_madison, -->
<!--              hmap_mesa,hmap_phoenix,hmap_pittsburgh,hmap_scottsdale, -->
<!--              hmap_tempe) -->
<!-- hmap_vec -->

<!-- category_demographics=function(hmap_vec,s,demographics){ -->
<!--   #values_vec = c(hmap_vec[1]$find(s),hmap_vec[2]$find(s), -->
<!--   #               hmap_vec[3]$find(s),hmap_vec[4]$find(s), -->
<!--   #               hmap_vec[5]$find(s),hmap_vec[6]$find(s), -->
<!--   #               hmap_vec[7]$find(s),hmap_vec[8]$find(s), -->
<!--   #               hmap_vec[9]$find(s),hmap_vec[10]$find(s), -->
<!--   #               hmap_vec[11]$find(s),hmap_vec[12]$find(s), -->
<!--   #               hmap_vec[13]$find(s),hmap_vec[14]$find(s)) -->
<!--   values_vec=c() -->
<!--   for(i in 1:length(hmap_vec)){ -->
<!--     values_vec = c(values_vec,hmap_vec[[i]]$find(s)) -->
<!--   } -->

<!--   return(values_vec) -->
<!-- } -->

<!-- test = category_demographics(hmap_vec,"Chinese",demographics) -->
<!-- test_reg = lm(test~demographics$asian) -->
<!-- summary(test_reg) -->
<!-- plot(demographics$asian,test) -->
<!-- lines(demographics$asian,test_reg$fitted.values) -->

<!-- #need to factor in %s -->
<!-- asian_percent = demographics$asian/demographics$pop_by_race_2015 -->
<!-- asian_percent -->

<!-- restaurants_usethis -->
<!-- cities_usethis -->
<!-- large_cities -->

<!-- city_restaurant_counts=sqldf("select large_cities.city, count from (large_cities join cities_usethis on large_cities.city=cities_usethis.city)") -->
<!-- city_restaurant_counts -->

<!-- restaurant_chinese_percent=test/city_restaurant_counts$count -->
<!-- restaurant_chinese_percent -->
<!-- percent_reg_test = lm(restaurant_chinese_percent~asian_percent) -->
<!-- summary(percent_reg_test) -->
<!-- percent_reg_test2 = lm(restaurant_chinese_percent~asian_percent+demographics$income_index+demographics$pop_growth) -->
<!-- summary(percent_reg_test2) -->
<!-- ``` -->


<!-- #analyze reviews -->

<!-- ```{r} -->
<!-- #subset reviews to match city data -->
<!-- reviews_usethis=sqldf("select review.stars, date, text, categories, city from review join restaurants_usethis on review.business_id=restaurants_usethis.business_id") -->

<!-- ``` -->




sources:
fred
yelp
https://www.bea.gov/iTable/iTable.cfm?reqid=9&step=1&acrdn=2#reqid=9&step=1&isuri=1&904=2004&903=64&906=q&905=2016&910=x&911=0


